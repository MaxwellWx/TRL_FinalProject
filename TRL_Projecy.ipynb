{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RtOLG_QGWTS5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd043910-66a1-4b08-df7c-c33d6a7a2815"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'imdb'...\n",
            "remote: Enumerating objects: 63, done.\u001b[K\n",
            "remote: Total 63 (delta 0), reused 0 (delta 0), pack-reused 63 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (63/63), 12.91 KiB | 777.00 KiB/s, done.\n",
            "Filtering content: 100% (3/3), 79.58 MiB | 13.82 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"-------------------------------clone repository-------------------------------\"\"\"\n",
        "\n",
        "\n",
        "#!git clone https://huggingface.co/google-bert/bert-base-uncased\n",
        "!git clone https://huggingface.co/datasets/stanfordnlp/imdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KnBSUp8Dy9j"
      },
      "outputs": [],
      "source": [
        "\"\"\"-------------------------------清理内存-------------------------------\"\"\"\n",
        "\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "import os\n",
        "#del model\n",
        "os._exit(00)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pxlnHYh2S4JM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ffb08ef-f4d7-4d9b-c9d7-1ff84b65a2d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\"\"\"-------------------------------定义训练模式、分类器-------------------------------\"\"\"\n",
        "\n",
        "\n",
        "MODEL_NAME = \"bert-base-uncased\"\n",
        "MODE = True # False for freezing,True otherwise\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "train_data = pd.read_parquet(\"/content/imdb/plain_text/train-00000-of-00001.parquet\")\n",
        "\n",
        "train_text = tokenizer(train_data['text'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
        "train_label = torch.tensor(train_data['label'].tolist())\n",
        "\n",
        "test_data = pd.read_parquet(\"/content/imdb/plain_text/test-00000-of-00001.parquet\")\n",
        "\n",
        "test_text = tokenizer(test_data['text'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
        "test_label = torch.tensor(test_data['label'].tolist())\n",
        "\n",
        "from transformers import BertModel, BertTokenizer, BertForSequenceClassification,AdamW\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from transformers import BertConfig\n",
        "\n",
        "\n",
        "class BertTextClassificationModel(nn.Module):\n",
        "    def __init__(self, num_classes=2, model_name=MODEL_NAME, dropout_prob=0.1,mode=MODE):\n",
        "        super(BertTextClassificationModel, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(dropout_prob)  # 添加dropout层以防止过拟合\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.bert.config.hidden_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "        # 冻结BERT模型的参数\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = mode\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = outputs.last_hidden_state\n",
        "        pooled_output = torch.mean(last_hidden_state, dim=1)\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "mnjmL9ccT0ln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10b5a50f-4704-4364-b9ee-eb851417bf4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\n",
            "count 32, Loss: 0.6818029880523682\n",
            "count 64, Loss: 0.7073661088943481\n",
            "count 96, Loss: 0.6769632697105408\n",
            "count 128, Loss: 0.7083679437637329\n",
            "count 160, Loss: 0.6946147084236145\n",
            "count 192, Loss: 0.6818351745605469\n",
            "count 224, Loss: 0.6617627143859863\n",
            "count 256, Loss: 0.692987322807312\n",
            "count 288, Loss: 0.6859220266342163\n",
            "count 320, Loss: 0.6529792547225952\n",
            "count 352, Loss: 0.65240478515625\n",
            "count 384, Loss: 0.6589126586914062\n",
            "count 416, Loss: 0.6040855646133423\n",
            "count 448, Loss: 0.5711455941200256\n",
            "count 480, Loss: 0.623327910900116\n",
            "count 512, Loss: 0.5696452856063843\n",
            "count 544, Loss: 0.6719508171081543\n",
            "count 576, Loss: 0.543320894241333\n",
            "count 608, Loss: 0.7647387385368347\n",
            "count 640, Loss: 0.6111541390419006\n",
            "count 672, Loss: 0.5128993391990662\n",
            "count 704, Loss: 0.7197791934013367\n",
            "count 736, Loss: 0.7881654500961304\n",
            "count 768, Loss: 0.705785870552063\n",
            "count 800, Loss: 0.6581024527549744\n",
            "count 832, Loss: 0.6364842653274536\n",
            "count 864, Loss: 0.6513118147850037\n",
            "count 896, Loss: 0.7274278402328491\n",
            "count 928, Loss: 0.5933671593666077\n",
            "count 960, Loss: 0.7212661504745483\n",
            "count 992, Loss: 0.6835058331489563\n",
            "count 1024, Loss: 0.7480969429016113\n",
            "count 1056, Loss: 0.615882396697998\n",
            "count 1088, Loss: 0.6438960433006287\n",
            "count 1120, Loss: 0.6494916677474976\n",
            "count 1152, Loss: 0.6401075720787048\n",
            "count 1184, Loss: 0.6404739022254944\n",
            "count 1216, Loss: 0.6638813614845276\n",
            "count 1248, Loss: 0.5944541096687317\n",
            "count 1280, Loss: 0.631886899471283\n",
            "count 1312, Loss: 0.5441817045211792\n",
            "count 1344, Loss: 0.5461745858192444\n",
            "count 1376, Loss: 0.6132605671882629\n",
            "count 1408, Loss: 0.5153742432594299\n",
            "count 1440, Loss: 0.5097898840904236\n",
            "count 1472, Loss: 0.3168979585170746\n",
            "count 1504, Loss: 0.5120006203651428\n",
            "count 1536, Loss: 0.3574053943157196\n",
            "count 1568, Loss: 0.41144973039627075\n",
            "count 1600, Loss: 0.4908865690231323\n",
            "count 1632, Loss: 0.5041396021842957\n",
            "count 1664, Loss: 0.5269808769226074\n",
            "count 1696, Loss: 0.42282402515411377\n",
            "count 1728, Loss: 0.47955143451690674\n",
            "count 1760, Loss: 0.5960538387298584\n",
            "count 1792, Loss: 0.43262800574302673\n",
            "count 1824, Loss: 0.26358741521835327\n",
            "count 1856, Loss: 0.8403318524360657\n",
            "count 1888, Loss: 0.448141485452652\n",
            "count 1920, Loss: 0.3689195215702057\n",
            "count 1952, Loss: 0.3486427068710327\n",
            "count 1984, Loss: 0.6755855679512024\n",
            "count 2016, Loss: 0.5458072423934937\n",
            "count 2048, Loss: 0.5367512702941895\n",
            "count 2080, Loss: 0.5384845733642578\n",
            "count 2112, Loss: 0.5302966833114624\n",
            "count 2144, Loss: 0.5025092363357544\n",
            "count 2176, Loss: 0.40755757689476013\n",
            "count 2208, Loss: 0.5604530572891235\n",
            "count 2240, Loss: 0.4628058969974518\n",
            "count 2272, Loss: 0.5690696835517883\n",
            "count 2304, Loss: 0.5205931663513184\n",
            "count 2336, Loss: 0.39369359612464905\n",
            "count 2368, Loss: 0.5022616982460022\n",
            "count 2400, Loss: 0.4168221354484558\n",
            "count 2432, Loss: 0.3860009014606476\n",
            "count 2464, Loss: 0.4690850079059601\n",
            "count 2496, Loss: 0.41520875692367554\n",
            "count 2528, Loss: 0.36900049448013306\n",
            "count 2560, Loss: 0.5355444550514221\n",
            "count 2592, Loss: 0.49531710147857666\n",
            "count 2624, Loss: 0.482505738735199\n",
            "count 2656, Loss: 0.5328457951545715\n",
            "count 2688, Loss: 0.38733476400375366\n",
            "count 2720, Loss: 0.31274348497390747\n",
            "count 2752, Loss: 0.27772167325019836\n",
            "count 2784, Loss: 0.4198526442050934\n",
            "count 2816, Loss: 0.3288367688655853\n",
            "count 2848, Loss: 0.34569910168647766\n",
            "count 2880, Loss: 0.2994321882724762\n",
            "count 2912, Loss: 0.3072672188282013\n",
            "count 2944, Loss: 0.3360845446586609\n",
            "count 2976, Loss: 0.27447569370269775\n",
            "count 3008, Loss: 0.6143447756767273\n",
            "count 3040, Loss: 0.3731221556663513\n",
            "count 3072, Loss: 0.4612656831741333\n",
            "count 3104, Loss: 0.5208048820495605\n",
            "count 3136, Loss: 0.28635847568511963\n",
            "count 3168, Loss: 0.5667093396186829\n",
            "count 3200, Loss: 0.2894267737865448\n",
            "count 3232, Loss: 0.5095411539077759\n",
            "count 3264, Loss: 0.5788677334785461\n",
            "count 3296, Loss: 0.4161379039287567\n",
            "count 3328, Loss: 0.338104784488678\n",
            "count 3360, Loss: 0.4134921729564667\n",
            "count 3392, Loss: 0.5968092679977417\n",
            "count 3424, Loss: 0.5393120646476746\n",
            "count 3456, Loss: 0.2886399030685425\n",
            "count 3488, Loss: 0.48238325119018555\n",
            "count 3520, Loss: 0.29874569177627563\n",
            "count 3552, Loss: 0.38435667753219604\n",
            "count 3584, Loss: 0.38397216796875\n",
            "count 3616, Loss: 0.3706231415271759\n",
            "count 3648, Loss: 0.3703536093235016\n",
            "count 3680, Loss: 0.46593666076660156\n",
            "count 3712, Loss: 0.22902190685272217\n",
            "count 3744, Loss: 0.29641637206077576\n",
            "count 3776, Loss: 0.24741628766059875\n",
            "count 3808, Loss: 0.2731393575668335\n",
            "count 3840, Loss: 0.5265524983406067\n",
            "count 3872, Loss: 0.44285887479782104\n",
            "count 3904, Loss: 0.48708024621009827\n",
            "count 3936, Loss: 0.2605173587799072\n",
            "count 3968, Loss: 0.5233721137046814\n",
            "count 4000, Loss: 0.5730222463607788\n",
            "count 4032, Loss: 0.40334466099739075\n",
            "count 4064, Loss: 0.35649144649505615\n",
            "count 4096, Loss: 0.3612743616104126\n",
            "count 4128, Loss: 0.34299013018608093\n",
            "count 4160, Loss: 0.40256357192993164\n",
            "count 4192, Loss: 0.39006417989730835\n",
            "count 4224, Loss: 0.3402763307094574\n",
            "count 4256, Loss: 0.31299787759780884\n",
            "count 4288, Loss: 0.22743327915668488\n",
            "count 4320, Loss: 0.3540390133857727\n",
            "count 4352, Loss: 0.2623395323753357\n",
            "count 4384, Loss: 0.34475642442703247\n",
            "count 4416, Loss: 0.1965763419866562\n",
            "count 4448, Loss: 0.35427922010421753\n",
            "count 4480, Loss: 0.2847331464290619\n",
            "count 4512, Loss: 0.46323296427726746\n",
            "count 4544, Loss: 0.28301987051963806\n",
            "count 4576, Loss: 0.353655606508255\n",
            "count 4608, Loss: 0.24511392414569855\n",
            "count 4640, Loss: 0.23166559636592865\n",
            "count 4672, Loss: 0.26742035150527954\n",
            "count 4704, Loss: 0.2852972149848938\n",
            "count 4736, Loss: 0.3187062442302704\n",
            "count 4768, Loss: 0.5680801272392273\n",
            "count 4800, Loss: 0.3809792101383209\n",
            "count 4832, Loss: 0.20522968471050262\n",
            "count 4864, Loss: 0.25699278712272644\n",
            "count 4896, Loss: 0.33803123235702515\n",
            "count 4928, Loss: 0.19415932893753052\n",
            "count 4960, Loss: 0.24364052712917328\n",
            "count 4992, Loss: 0.22080719470977783\n",
            "count 5024, Loss: 0.3491652309894562\n",
            "count 5056, Loss: 0.5903065800666809\n",
            "count 5088, Loss: 0.4248693585395813\n",
            "count 5120, Loss: 0.3466998040676117\n",
            "count 5152, Loss: 0.2656933665275574\n",
            "count 5184, Loss: 0.4538288712501526\n",
            "count 5216, Loss: 0.3602478504180908\n",
            "count 5248, Loss: 0.4240393042564392\n",
            "count 5280, Loss: 0.4196590781211853\n",
            "count 5312, Loss: 0.287512868642807\n",
            "count 5344, Loss: 0.21034657955169678\n",
            "count 5376, Loss: 0.31043124198913574\n",
            "count 5408, Loss: 0.3797227144241333\n",
            "count 5440, Loss: 0.4901786148548126\n",
            "count 5472, Loss: 0.2574431300163269\n",
            "count 5504, Loss: 0.46423739194869995\n",
            "count 5536, Loss: 0.5439952611923218\n",
            "count 5568, Loss: 0.2728061378002167\n",
            "count 5600, Loss: 0.2843295931816101\n",
            "count 5632, Loss: 0.3471803069114685\n",
            "count 5664, Loss: 0.3184112012386322\n",
            "count 5696, Loss: 0.5810967683792114\n",
            "count 5728, Loss: 0.2851186692714691\n",
            "count 5760, Loss: 0.30050376057624817\n",
            "count 5792, Loss: 0.34261268377304077\n",
            "count 5824, Loss: 0.3040144443511963\n",
            "count 5856, Loss: 0.3712572753429413\n",
            "count 5888, Loss: 0.2265537977218628\n",
            "count 5920, Loss: 0.3730281889438629\n",
            "count 5952, Loss: 0.419752836227417\n",
            "count 5984, Loss: 0.32662278413772583\n",
            "count 6016, Loss: 0.28138282895088196\n",
            "count 6048, Loss: 0.22658085823059082\n",
            "count 6080, Loss: 0.3668060898780823\n",
            "count 6112, Loss: 0.22102417051792145\n",
            "count 6144, Loss: 0.5081741213798523\n",
            "count 6176, Loss: 0.3379407227039337\n",
            "count 6208, Loss: 0.20655475556850433\n",
            "count 6240, Loss: 0.30415230989456177\n",
            "count 6272, Loss: 0.4740780293941498\n",
            "count 6304, Loss: 0.41872814297676086\n",
            "count 6336, Loss: 0.3744851052761078\n",
            "count 6368, Loss: 0.3801155388355255\n",
            "count 6400, Loss: 0.27602437138557434\n",
            "count 6432, Loss: 0.3659522831439972\n",
            "count 6464, Loss: 0.27118980884552\n",
            "count 6496, Loss: 0.35920122265815735\n",
            "count 6528, Loss: 0.30499035120010376\n",
            "count 6560, Loss: 0.2959044873714447\n",
            "count 6592, Loss: 0.38974857330322266\n",
            "count 6624, Loss: 0.1277838945388794\n",
            "count 6656, Loss: 0.52425217628479\n",
            "count 6688, Loss: 0.299633651971817\n",
            "count 6720, Loss: 0.40997034311294556\n",
            "count 6752, Loss: 0.15192344784736633\n",
            "count 6784, Loss: 0.24979379773139954\n",
            "count 6816, Loss: 0.2646031379699707\n",
            "count 6848, Loss: 0.3412448763847351\n",
            "count 6880, Loss: 0.529121458530426\n",
            "count 6912, Loss: 0.14060327410697937\n",
            "count 6944, Loss: 0.16127485036849976\n",
            "count 6976, Loss: 0.263153076171875\n",
            "count 7008, Loss: 0.5415394306182861\n",
            "count 7040, Loss: 0.3167404532432556\n",
            "count 7072, Loss: 0.283038854598999\n",
            "count 7104, Loss: 0.19411908090114594\n",
            "count 7136, Loss: 0.4042837917804718\n",
            "count 7168, Loss: 0.2362823188304901\n",
            "count 7200, Loss: 0.16751457750797272\n",
            "count 7232, Loss: 0.21110805869102478\n",
            "count 7264, Loss: 0.29872816801071167\n",
            "count 7296, Loss: 0.36336401104927063\n",
            "count 7328, Loss: 0.16918659210205078\n",
            "count 7360, Loss: 0.2431868612766266\n",
            "count 7392, Loss: 0.2929726243019104\n",
            "count 7424, Loss: 0.5590190887451172\n",
            "count 7456, Loss: 0.1893339902162552\n",
            "count 7488, Loss: 0.3147106468677521\n",
            "count 7520, Loss: 0.5439662933349609\n",
            "count 7552, Loss: 0.5505771040916443\n",
            "count 7584, Loss: 0.18807782232761383\n",
            "count 7616, Loss: 0.1987757682800293\n",
            "count 7648, Loss: 0.4610801339149475\n",
            "count 7680, Loss: 0.18144652247428894\n",
            "count 7712, Loss: 0.3339225649833679\n",
            "count 7744, Loss: 0.4269818365573883\n",
            "count 7776, Loss: 0.319336861371994\n",
            "count 7808, Loss: 0.2568311393260956\n",
            "count 7840, Loss: 0.24153389036655426\n",
            "count 7872, Loss: 0.17728117108345032\n",
            "count 7904, Loss: 0.14892986416816711\n",
            "count 7936, Loss: 0.287689745426178\n",
            "count 7968, Loss: 0.370491087436676\n",
            "count 8000, Loss: 0.22617214918136597\n",
            "count 8032, Loss: 0.2255549132823944\n",
            "count 8064, Loss: 0.2330659180879593\n",
            "count 8096, Loss: 0.38530367612838745\n",
            "count 8128, Loss: 0.23562124371528625\n",
            "count 8160, Loss: 0.303057998418808\n",
            "count 8192, Loss: 0.3762577772140503\n",
            "count 8224, Loss: 0.43502894043922424\n",
            "count 8256, Loss: 0.3507116436958313\n",
            "count 8288, Loss: 0.24266687035560608\n",
            "count 8320, Loss: 0.3667675852775574\n",
            "count 8352, Loss: 0.457011342048645\n",
            "count 8384, Loss: 0.44772976636886597\n",
            "count 8416, Loss: 0.2606310546398163\n",
            "count 8448, Loss: 0.2897012531757355\n",
            "count 8480, Loss: 0.20258963108062744\n",
            "count 8512, Loss: 0.20385019481182098\n",
            "count 8544, Loss: 0.29124996066093445\n",
            "count 8576, Loss: 0.35303106904029846\n",
            "count 8608, Loss: 0.3310362696647644\n",
            "count 8640, Loss: 0.4229258596897125\n",
            "count 8672, Loss: 0.5246533155441284\n",
            "count 8704, Loss: 0.4458702504634857\n",
            "count 8736, Loss: 0.1986360251903534\n",
            "count 8768, Loss: 0.22774995863437653\n",
            "count 8800, Loss: 0.2791769802570343\n",
            "count 8832, Loss: 0.30813440680503845\n",
            "count 8864, Loss: 0.47109588980674744\n",
            "count 8896, Loss: 0.3295958638191223\n",
            "count 8928, Loss: 0.2972123622894287\n",
            "count 8960, Loss: 0.19741976261138916\n",
            "count 8992, Loss: 0.3999255895614624\n",
            "count 9024, Loss: 0.23116101324558258\n",
            "count 9056, Loss: 0.30430811643600464\n",
            "count 9088, Loss: 0.3520365059375763\n",
            "count 9120, Loss: 0.31132546067237854\n",
            "count 9152, Loss: 0.4291324019432068\n",
            "count 9184, Loss: 0.21434833109378815\n",
            "count 9216, Loss: 0.3267565667629242\n",
            "count 9248, Loss: 0.3051062822341919\n",
            "count 9280, Loss: 0.20125874876976013\n",
            "count 9312, Loss: 0.26636046171188354\n",
            "count 9344, Loss: 0.17181015014648438\n",
            "count 9376, Loss: 0.306331992149353\n",
            "count 9408, Loss: 0.31072887778282166\n",
            "count 9440, Loss: 0.42071834206581116\n",
            "count 9472, Loss: 0.3127591907978058\n",
            "count 9504, Loss: 0.3382982611656189\n",
            "count 9536, Loss: 0.3891802728176117\n",
            "count 9568, Loss: 0.3689834773540497\n",
            "count 9600, Loss: 0.31232571601867676\n",
            "count 9632, Loss: 0.31159549951553345\n",
            "count 9664, Loss: 0.3352828919887543\n",
            "count 9696, Loss: 0.1513177454471588\n",
            "count 9728, Loss: 0.265034943819046\n",
            "count 9760, Loss: 0.13263872265815735\n",
            "count 9792, Loss: 0.1594720184803009\n",
            "count 9824, Loss: 0.27664318680763245\n",
            "count 9856, Loss: 0.38070642948150635\n",
            "count 9888, Loss: 0.13723643124103546\n",
            "count 9920, Loss: 0.37957125902175903\n",
            "count 9952, Loss: 0.31311240792274475\n",
            "count 9984, Loss: 0.43826913833618164\n",
            "count 10016, Loss: 0.34249672293663025\n",
            "count 10048, Loss: 0.3189677298069\n",
            "count 10080, Loss: 0.28653091192245483\n",
            "count 10112, Loss: 0.37643977999687195\n",
            "count 10144, Loss: 0.7857047915458679\n",
            "count 10176, Loss: 0.20276229083538055\n",
            "count 10208, Loss: 0.24772100150585175\n",
            "count 10240, Loss: 0.3034198582172394\n",
            "count 10272, Loss: 0.2315540611743927\n",
            "count 10304, Loss: 0.22823502123355865\n",
            "count 10336, Loss: 0.2889610528945923\n",
            "count 10368, Loss: 0.46544769406318665\n",
            "count 10400, Loss: 0.1900661140680313\n",
            "count 10432, Loss: 0.41491299867630005\n",
            "count 10464, Loss: 0.3062095046043396\n",
            "count 10496, Loss: 0.2236640453338623\n",
            "count 10528, Loss: 0.20487378537654877\n",
            "count 10560, Loss: 0.21543057262897491\n",
            "count 10592, Loss: 0.12581688165664673\n",
            "count 10624, Loss: 0.2873532474040985\n",
            "count 10656, Loss: 0.27955278754234314\n",
            "count 10688, Loss: 0.24729318916797638\n",
            "count 10720, Loss: 0.1193917766213417\n",
            "count 10752, Loss: 0.12700575590133667\n",
            "count 10784, Loss: 0.15822260081768036\n",
            "count 10816, Loss: 0.07334794849157333\n",
            "count 10848, Loss: 0.05491132661700249\n",
            "count 10880, Loss: 0.5183118581771851\n",
            "count 10912, Loss: 0.2737528085708618\n",
            "count 10944, Loss: 0.570364773273468\n",
            "count 10976, Loss: 0.7777041792869568\n",
            "count 11008, Loss: 0.5753070116043091\n",
            "count 11040, Loss: 0.15362393856048584\n",
            "count 11072, Loss: 0.21795368194580078\n",
            "count 11104, Loss: 0.46523937582969666\n",
            "count 11136, Loss: 0.24444034695625305\n",
            "count 11168, Loss: 0.3528639078140259\n",
            "count 11200, Loss: 0.4755754768848419\n",
            "count 11232, Loss: 0.30773216485977173\n",
            "count 11264, Loss: 0.38789835572242737\n",
            "count 11296, Loss: 0.1881224513053894\n",
            "count 11328, Loss: 0.433628648519516\n",
            "count 11360, Loss: 0.45879942178726196\n",
            "count 11392, Loss: 0.4108000695705414\n",
            "count 11424, Loss: 0.2747695744037628\n",
            "count 11456, Loss: 0.3454045057296753\n",
            "count 11488, Loss: 0.28048375248908997\n",
            "count 11520, Loss: 0.379170686006546\n",
            "count 11552, Loss: 0.3059995472431183\n",
            "count 11584, Loss: 0.2413923591375351\n",
            "count 11616, Loss: 0.2747041583061218\n",
            "count 11648, Loss: 0.1498989462852478\n",
            "count 11680, Loss: 0.17588649690151215\n",
            "count 11712, Loss: 0.12637552618980408\n",
            "count 11744, Loss: 0.16766926646232605\n",
            "count 11776, Loss: 0.07052867859601974\n",
            "count 11808, Loss: 0.33846935629844666\n",
            "count 11840, Loss: 0.2985359728336334\n",
            "count 11872, Loss: 0.2551944851875305\n",
            "count 11904, Loss: 0.26933935284614563\n",
            "count 11936, Loss: 0.3754127025604248\n",
            "count 11968, Loss: 0.49582597613334656\n",
            "count 12000, Loss: 0.39609336853027344\n",
            "count 12032, Loss: 0.3521609604358673\n",
            "count 12064, Loss: 0.303516685962677\n",
            "count 12096, Loss: 0.27980706095695496\n",
            "count 12128, Loss: 0.18632303178310394\n",
            "count 12160, Loss: 0.22958895564079285\n",
            "count 12192, Loss: 0.5127288699150085\n",
            "count 12224, Loss: 0.22826054692268372\n",
            "count 12256, Loss: 0.187770813703537\n",
            "count 12288, Loss: 0.29548048973083496\n",
            "count 12320, Loss: 0.22662250697612762\n",
            "count 12352, Loss: 0.16475722193717957\n",
            "count 12384, Loss: 0.30372536182403564\n",
            "count 12416, Loss: 0.34025704860687256\n",
            "count 12448, Loss: 0.35012340545654297\n",
            "count 12480, Loss: 0.2706529498100281\n",
            "count 12512, Loss: 0.2107987254858017\n",
            "count 12544, Loss: 0.36368992924690247\n",
            "count 12576, Loss: 0.33432328701019287\n",
            "count 12608, Loss: 0.2990185618400574\n",
            "count 12640, Loss: 0.23528407514095306\n",
            "count 12672, Loss: 0.29465529322624207\n",
            "count 12704, Loss: 0.22421032190322876\n",
            "count 12736, Loss: 0.4519445598125458\n",
            "count 12768, Loss: 0.20873022079467773\n",
            "count 12800, Loss: 0.29405921697616577\n",
            "count 12832, Loss: 0.1958024799823761\n",
            "count 12864, Loss: 0.15408816933631897\n",
            "count 12896, Loss: 0.3341893255710602\n",
            "count 12928, Loss: 0.2767033576965332\n",
            "count 12960, Loss: 0.21226966381072998\n",
            "count 12992, Loss: 0.3223085403442383\n",
            "count 13024, Loss: 0.3937654495239258\n",
            "count 13056, Loss: 0.3269471228122711\n",
            "count 13088, Loss: 0.2887952923774719\n",
            "count 13120, Loss: 0.2961726784706116\n",
            "count 13152, Loss: 0.1215922012925148\n",
            "count 13184, Loss: 0.24982500076293945\n",
            "count 13216, Loss: 0.28873756527900696\n",
            "count 13248, Loss: 0.26399338245391846\n",
            "count 13280, Loss: 0.17936548590660095\n",
            "count 13312, Loss: 0.5548001527786255\n",
            "count 13344, Loss: 0.2505679726600647\n",
            "count 13376, Loss: 0.3975723683834076\n",
            "count 13408, Loss: 0.304159551858902\n",
            "count 13440, Loss: 0.34397685527801514\n",
            "count 13472, Loss: 0.27132105827331543\n",
            "count 13504, Loss: 0.11902816593647003\n",
            "count 13536, Loss: 0.17621538043022156\n",
            "count 13568, Loss: 0.07588605582714081\n",
            "count 13600, Loss: 0.4037666916847229\n",
            "count 13632, Loss: 0.2245861142873764\n",
            "count 13664, Loss: 0.3240506649017334\n",
            "count 13696, Loss: 0.11643202602863312\n",
            "count 13728, Loss: 0.21973226964473724\n",
            "count 13760, Loss: 0.31845584511756897\n",
            "count 13792, Loss: 0.46689140796661377\n",
            "count 13824, Loss: 0.30575329065322876\n",
            "count 13856, Loss: 0.2940041422843933\n",
            "count 13888, Loss: 0.3103584051132202\n",
            "count 13920, Loss: 0.2704375088214874\n",
            "count 13952, Loss: 0.3680915832519531\n",
            "count 13984, Loss: 0.2096535712480545\n",
            "count 14016, Loss: 0.31468555331230164\n",
            "count 14048, Loss: 0.1336599439382553\n",
            "count 14080, Loss: 0.2722227871417999\n",
            "count 14112, Loss: 0.29793888330459595\n",
            "count 14144, Loss: 0.2921346127986908\n",
            "count 14176, Loss: 0.18540403246879578\n",
            "count 14208, Loss: 0.1837293803691864\n",
            "count 14240, Loss: 0.21217581629753113\n",
            "count 14272, Loss: 0.27332744002342224\n",
            "count 14304, Loss: 0.31581127643585205\n",
            "count 14336, Loss: 0.33915889263153076\n",
            "count 14368, Loss: 0.14600926637649536\n",
            "count 14400, Loss: 0.3093428909778595\n",
            "count 14432, Loss: 0.34054890275001526\n",
            "count 14464, Loss: 0.13244594633579254\n",
            "count 14496, Loss: 0.3644489347934723\n",
            "count 14528, Loss: 0.3311142027378082\n",
            "count 14560, Loss: 0.26955702900886536\n",
            "count 14592, Loss: 0.4702664017677307\n",
            "count 14624, Loss: 0.17109662294387817\n",
            "count 14656, Loss: 0.15462446212768555\n",
            "count 14688, Loss: 0.3636850416660309\n",
            "count 14720, Loss: 0.40348148345947266\n",
            "count 14752, Loss: 0.2313327044248581\n",
            "count 14784, Loss: 0.2986769676208496\n",
            "count 14816, Loss: 0.1526094526052475\n",
            "count 14848, Loss: 0.19069892168045044\n",
            "count 14880, Loss: 0.3282342553138733\n",
            "count 14912, Loss: 0.32333266735076904\n",
            "count 14944, Loss: 0.13371498882770538\n",
            "count 14976, Loss: 0.18534107506275177\n",
            "count 15008, Loss: 0.27472200989723206\n",
            "count 15040, Loss: 0.3344743251800537\n",
            "count 15072, Loss: 0.06805519014596939\n",
            "count 15104, Loss: 0.4214894771575928\n",
            "count 15136, Loss: 0.1699356734752655\n",
            "count 15168, Loss: 0.179845929145813\n",
            "count 15200, Loss: 0.16010065376758575\n",
            "count 15232, Loss: 0.3265567719936371\n",
            "count 15264, Loss: 0.3588636517524719\n",
            "count 15296, Loss: 0.24070529639720917\n",
            "count 15328, Loss: 0.2244221270084381\n",
            "count 15360, Loss: 0.37203726172447205\n",
            "count 15392, Loss: 0.3576277494430542\n",
            "count 15424, Loss: 0.35214120149612427\n",
            "count 15456, Loss: 0.27646005153656006\n",
            "count 15488, Loss: 0.19019615650177002\n",
            "count 15520, Loss: 0.25506192445755005\n",
            "count 15552, Loss: 0.26232871413230896\n",
            "count 15584, Loss: 0.36325153708457947\n",
            "count 15616, Loss: 0.18564866483211517\n",
            "count 15648, Loss: 0.41425225138664246\n",
            "count 15680, Loss: 0.2830646336078644\n",
            "count 15712, Loss: 0.35391125082969666\n",
            "count 15744, Loss: 0.41669610142707825\n",
            "count 15776, Loss: 0.21716760098934174\n",
            "count 15808, Loss: 0.3042963743209839\n",
            "count 15840, Loss: 0.15474073588848114\n",
            "count 15872, Loss: 0.23632195591926575\n",
            "count 15904, Loss: 0.24975493550300598\n",
            "count 15936, Loss: 0.25611087679862976\n",
            "count 15968, Loss: 0.3087122142314911\n",
            "count 16000, Loss: 0.26098790764808655\n",
            "count 16032, Loss: 0.2131899893283844\n",
            "count 16064, Loss: 0.18766961991786957\n",
            "count 16096, Loss: 0.1827094703912735\n",
            "count 16128, Loss: 0.12322784960269928\n",
            "count 16160, Loss: 0.4376457929611206\n",
            "count 16192, Loss: 0.31586024165153503\n",
            "count 16224, Loss: 0.20351152122020721\n",
            "count 16256, Loss: 0.44932061433792114\n",
            "count 16288, Loss: 0.14320828020572662\n",
            "count 16320, Loss: 0.46388086676597595\n",
            "count 16352, Loss: 0.4563107490539551\n",
            "count 16384, Loss: 0.2122378945350647\n",
            "count 16416, Loss: 0.14989805221557617\n",
            "count 16448, Loss: 0.44962257146835327\n",
            "count 16480, Loss: 0.174778550863266\n",
            "count 16512, Loss: 0.34239062666893005\n",
            "count 16544, Loss: 0.26075994968414307\n",
            "count 16576, Loss: 0.379625141620636\n",
            "count 16608, Loss: 0.3276195824146271\n",
            "count 16640, Loss: 0.2677326798439026\n",
            "count 16672, Loss: 0.47499769926071167\n",
            "count 16704, Loss: 0.24804005026817322\n",
            "count 16736, Loss: 0.25047725439071655\n",
            "count 16768, Loss: 0.18591660261154175\n",
            "count 16800, Loss: 0.29336756467819214\n",
            "count 16832, Loss: 0.12911279499530792\n",
            "count 16864, Loss: 0.29224082827568054\n",
            "count 16896, Loss: 0.31713542342185974\n",
            "count 16928, Loss: 0.22709472477436066\n",
            "count 16960, Loss: 0.2922481298446655\n",
            "count 16992, Loss: 0.28960946202278137\n",
            "count 17024, Loss: 0.1698862761259079\n",
            "count 17056, Loss: 0.3327919542789459\n",
            "count 17088, Loss: 0.3224376440048218\n",
            "count 17120, Loss: 0.12930043041706085\n",
            "count 17152, Loss: 0.30166926980018616\n",
            "count 17184, Loss: 0.16421416401863098\n",
            "count 17216, Loss: 0.5315995216369629\n",
            "count 17248, Loss: 0.16902785003185272\n",
            "count 17280, Loss: 0.21469245851039886\n",
            "count 17312, Loss: 0.24938519299030304\n",
            "count 17344, Loss: 0.31078261137008667\n",
            "count 17376, Loss: 0.17465819418430328\n",
            "count 17408, Loss: 0.2642461061477661\n",
            "count 17440, Loss: 0.2484733611345291\n",
            "count 17472, Loss: 0.274314284324646\n",
            "count 17504, Loss: 0.3469844162464142\n",
            "count 17536, Loss: 0.29966577887535095\n",
            "count 17568, Loss: 0.14369814097881317\n",
            "count 17600, Loss: 0.19344112277030945\n",
            "count 17632, Loss: 0.1678517609834671\n",
            "count 17664, Loss: 0.4385080337524414\n",
            "count 17696, Loss: 0.32416677474975586\n",
            "count 17728, Loss: 0.43396005034446716\n",
            "count 17760, Loss: 0.6356719732284546\n",
            "count 17792, Loss: 0.3457147479057312\n",
            "count 17824, Loss: 0.2084028273820877\n",
            "count 17856, Loss: 0.4024680554866791\n",
            "count 17888, Loss: 0.35757556557655334\n",
            "count 17920, Loss: 0.2934703528881073\n",
            "count 17952, Loss: 0.2811889052391052\n",
            "count 17984, Loss: 0.30606916546821594\n",
            "count 18016, Loss: 0.3326450288295746\n",
            "count 18048, Loss: 0.2856857478618622\n",
            "count 18080, Loss: 0.21536239981651306\n",
            "count 18112, Loss: 0.4357641041278839\n",
            "count 18144, Loss: 0.17385737597942352\n",
            "count 18176, Loss: 0.13395389914512634\n",
            "count 18208, Loss: 0.13517118990421295\n",
            "count 18240, Loss: 0.1790202409029007\n",
            "count 18272, Loss: 0.16647180914878845\n",
            "count 18304, Loss: 0.3374043107032776\n",
            "count 18336, Loss: 0.3661330044269562\n",
            "count 18368, Loss: 0.22340582311153412\n",
            "count 18400, Loss: 0.41770583391189575\n",
            "count 18432, Loss: 0.13710784912109375\n",
            "count 18464, Loss: 0.24264183640480042\n",
            "count 18496, Loss: 0.3255963921546936\n",
            "count 18528, Loss: 0.18988999724388123\n",
            "count 18560, Loss: 0.393553227186203\n",
            "count 18592, Loss: 0.20737378299236298\n",
            "count 18624, Loss: 0.36003556847572327\n",
            "count 18656, Loss: 0.2877900004386902\n",
            "count 18688, Loss: 0.1869574636220932\n",
            "count 18720, Loss: 0.1579042226076126\n",
            "count 18752, Loss: 0.24297939240932465\n",
            "count 18784, Loss: 0.6067222952842712\n",
            "count 18816, Loss: 0.2787119448184967\n",
            "count 18848, Loss: 0.20726853609085083\n",
            "count 18880, Loss: 0.16396528482437134\n",
            "count 18912, Loss: 0.2871648371219635\n",
            "count 18944, Loss: 0.4607258141040802\n",
            "count 18976, Loss: 0.38802221417427063\n",
            "count 19008, Loss: 0.12027744948863983\n",
            "count 19040, Loss: 0.3977550268173218\n",
            "count 19072, Loss: 0.26824524998664856\n",
            "count 19104, Loss: 0.28701871633529663\n",
            "count 19136, Loss: 0.2938062846660614\n",
            "count 19168, Loss: 0.1762784719467163\n",
            "count 19200, Loss: 0.23023000359535217\n",
            "count 19232, Loss: 0.471643328666687\n",
            "count 19264, Loss: 0.19310340285301208\n",
            "count 19296, Loss: 0.14732134342193604\n",
            "count 19328, Loss: 0.22369793057441711\n",
            "count 19360, Loss: 0.19006213545799255\n",
            "count 19392, Loss: 0.10292360186576843\n",
            "count 19424, Loss: 0.24010731279850006\n",
            "count 19456, Loss: 0.19239287078380585\n",
            "count 19488, Loss: 0.18032589554786682\n",
            "count 19520, Loss: 0.2789577841758728\n",
            "count 19552, Loss: 0.21456366777420044\n",
            "count 19584, Loss: 0.3538333475589752\n",
            "count 19616, Loss: 0.22520962357521057\n",
            "count 19648, Loss: 0.3921051621437073\n",
            "count 19680, Loss: 0.32594388723373413\n",
            "count 19712, Loss: 0.22903114557266235\n",
            "count 19744, Loss: 0.4556633234024048\n",
            "count 19776, Loss: 0.23322758078575134\n",
            "count 19808, Loss: 0.088103748857975\n",
            "count 19840, Loss: 0.2843135893344879\n",
            "count 19872, Loss: 0.19644702970981598\n",
            "count 19904, Loss: 0.1927625685930252\n",
            "count 19936, Loss: 0.31004655361175537\n",
            "count 19968, Loss: 0.2962329685688019\n",
            "count 20000, Loss: 0.4301779270172119\n",
            "count 20032, Loss: 0.11560927331447601\n",
            "count 20064, Loss: 0.2536771297454834\n",
            "count 20096, Loss: 0.27671629190444946\n",
            "count 20128, Loss: 0.20946002006530762\n",
            "count 20160, Loss: 0.22818692028522491\n",
            "count 20192, Loss: 0.2027183175086975\n",
            "count 20224, Loss: 0.1489332765340805\n",
            "count 20256, Loss: 0.2553918659687042\n",
            "count 20288, Loss: 0.28266045451164246\n",
            "count 20320, Loss: 0.28570425510406494\n",
            "count 20352, Loss: 0.2764139175415039\n",
            "count 20384, Loss: 0.3637137711048126\n",
            "count 20416, Loss: 0.4075731933116913\n",
            "count 20448, Loss: 0.2032128870487213\n",
            "count 20480, Loss: 0.3244905173778534\n",
            "count 20512, Loss: 0.18877600133419037\n",
            "count 20544, Loss: 0.29216286540031433\n",
            "count 20576, Loss: 0.3222886323928833\n",
            "count 20608, Loss: 0.22591182589530945\n",
            "count 20640, Loss: 0.22178086638450623\n",
            "count 20672, Loss: 0.22559167444705963\n",
            "count 20704, Loss: 0.3454316556453705\n",
            "count 20736, Loss: 0.2801516354084015\n",
            "count 20768, Loss: 0.3094465732574463\n",
            "count 20800, Loss: 0.1848025768995285\n",
            "count 20832, Loss: 0.33878326416015625\n",
            "count 20864, Loss: 0.2052280157804489\n",
            "count 20896, Loss: 0.2125268578529358\n",
            "count 20928, Loss: 0.21418112516403198\n",
            "count 20960, Loss: 0.3565882444381714\n",
            "count 20992, Loss: 0.3050142228603363\n",
            "count 21024, Loss: 0.48784339427948\n",
            "count 21056, Loss: 0.1592063158750534\n",
            "count 21088, Loss: 0.24972054362297058\n",
            "count 21120, Loss: 0.33789747953414917\n",
            "count 21152, Loss: 0.26470887660980225\n",
            "count 21184, Loss: 0.21854358911514282\n",
            "count 21216, Loss: 0.15899062156677246\n",
            "count 21248, Loss: 0.19075946509838104\n",
            "count 21280, Loss: 0.3935898244380951\n",
            "count 21312, Loss: 0.29226624965667725\n",
            "count 21344, Loss: 0.4830757677555084\n",
            "count 21376, Loss: 0.24167615175247192\n",
            "count 21408, Loss: 0.2526758909225464\n",
            "count 21440, Loss: 0.2559930980205536\n",
            "count 21472, Loss: 0.15363569557666779\n",
            "count 21504, Loss: 0.11495257914066315\n",
            "count 21536, Loss: 0.12709173560142517\n",
            "count 21568, Loss: 0.2142842561006546\n",
            "count 21600, Loss: 0.20397110283374786\n",
            "count 21632, Loss: 0.2765466868877411\n",
            "count 21664, Loss: 0.3015528619289398\n",
            "count 21696, Loss: 0.23971886932849884\n",
            "count 21728, Loss: 0.23280225694179535\n",
            "count 21760, Loss: 0.16806890070438385\n",
            "count 21792, Loss: 0.08805248141288757\n",
            "count 21824, Loss: 0.27588748931884766\n",
            "count 21856, Loss: 0.3206130266189575\n",
            "count 21888, Loss: 0.40743640065193176\n",
            "count 21920, Loss: 0.28129032254219055\n",
            "count 21952, Loss: 0.23119443655014038\n",
            "count 21984, Loss: 0.1763390749692917\n",
            "count 22016, Loss: 0.19615735113620758\n",
            "count 22048, Loss: 0.1206478700041771\n",
            "count 22080, Loss: 0.18554943799972534\n",
            "count 22112, Loss: 0.3435021936893463\n",
            "count 22144, Loss: 0.18038176000118256\n",
            "count 22176, Loss: 0.19983553886413574\n",
            "count 22208, Loss: 0.43424251675605774\n",
            "count 22240, Loss: 0.15458256006240845\n",
            "count 22272, Loss: 0.3037263751029968\n",
            "count 22304, Loss: 0.0884062796831131\n",
            "count 22336, Loss: 0.33035168051719666\n",
            "count 22368, Loss: 0.06253108382225037\n",
            "count 22400, Loss: 0.19875545799732208\n",
            "count 22432, Loss: 0.4018877446651459\n",
            "count 22464, Loss: 0.3687604069709778\n",
            "count 22496, Loss: 0.21019741892814636\n",
            "count 22528, Loss: 0.3918040990829468\n",
            "count 22560, Loss: 0.4845365881919861\n",
            "count 22592, Loss: 0.3033542037010193\n",
            "count 22624, Loss: 0.16950573027133942\n",
            "count 22656, Loss: 0.4932921826839447\n",
            "count 22688, Loss: 0.4457056224346161\n",
            "count 22720, Loss: 0.3565422594547272\n",
            "count 22752, Loss: 0.2133323848247528\n",
            "count 22784, Loss: 0.41113170981407166\n",
            "count 22816, Loss: 0.22035551071166992\n",
            "count 22848, Loss: 0.29880616068840027\n",
            "count 22880, Loss: 0.21950922906398773\n",
            "count 22912, Loss: 0.28431257605552673\n",
            "count 22944, Loss: 0.22994676232337952\n",
            "count 22976, Loss: 0.31353792548179626\n",
            "count 23008, Loss: 0.12036598473787308\n",
            "count 23040, Loss: 0.14029282331466675\n",
            "count 23072, Loss: 0.11631865054368973\n",
            "count 23104, Loss: 0.303762286901474\n",
            "count 23136, Loss: 0.23793551325798035\n",
            "count 23168, Loss: 0.149685338139534\n",
            "count 23200, Loss: 0.2271307110786438\n",
            "count 23232, Loss: 0.07690557837486267\n",
            "count 23264, Loss: 0.27155956625938416\n",
            "count 23296, Loss: 0.2966756224632263\n",
            "count 23328, Loss: 0.1349761039018631\n",
            "count 23360, Loss: 0.43069177865982056\n",
            "count 23392, Loss: 0.10555852949619293\n",
            "count 23424, Loss: 0.3485419750213623\n",
            "count 23456, Loss: 0.21434363722801208\n",
            "count 23488, Loss: 0.14176766574382782\n",
            "count 23520, Loss: 0.06495169550180435\n",
            "count 23552, Loss: 0.06323853880167007\n",
            "count 23584, Loss: 0.1531628966331482\n",
            "count 23616, Loss: 0.06942062079906464\n",
            "count 23648, Loss: 0.3027245104312897\n",
            "count 23680, Loss: 0.19133974611759186\n",
            "count 23712, Loss: 0.13324449956417084\n",
            "count 23744, Loss: 0.12403145432472229\n",
            "count 23776, Loss: 0.504815936088562\n",
            "count 23808, Loss: 0.09042610973119736\n",
            "count 23840, Loss: 0.4386652410030365\n",
            "count 23872, Loss: 0.47203558683395386\n",
            "count 23904, Loss: 0.21205516159534454\n",
            "count 23936, Loss: 0.27158084511756897\n",
            "count 23968, Loss: 0.18439750373363495\n",
            "count 24000, Loss: 0.3775253891944885\n",
            "count 24032, Loss: 0.19283604621887207\n",
            "count 24064, Loss: 0.1507803350687027\n",
            "count 24096, Loss: 0.29134270548820496\n",
            "count 24128, Loss: 0.3389526307582855\n",
            "count 24160, Loss: 0.16360622644424438\n",
            "count 24192, Loss: 0.2330583930015564\n",
            "count 24224, Loss: 0.17702391743659973\n",
            "count 24256, Loss: 0.16808202862739563\n",
            "count 24288, Loss: 0.23120133578777313\n",
            "count 24320, Loss: 0.19406816363334656\n",
            "count 24352, Loss: 0.1759355664253235\n",
            "count 24384, Loss: 0.4197331666946411\n",
            "count 24416, Loss: 0.3159715533256531\n",
            "count 24448, Loss: 0.10861458629369736\n",
            "count 24480, Loss: 0.16409088671207428\n",
            "count 24512, Loss: 0.21033622324466705\n",
            "count 24544, Loss: 0.0665082260966301\n",
            "count 24576, Loss: 0.2155362367630005\n",
            "count 24608, Loss: 0.27872589230537415\n",
            "count 24640, Loss: 0.27350980043411255\n",
            "count 24672, Loss: 0.1383098065853119\n",
            "count 24704, Loss: 0.1318741738796234\n",
            "count 24736, Loss: 0.13863228261470795\n",
            "count 24768, Loss: 0.06973472237586975\n",
            "count 24800, Loss: 0.10280279070138931\n",
            "count 24832, Loss: 0.2913479804992676\n",
            "count 24864, Loss: 0.31429025530815125\n",
            "count 24896, Loss: 0.42663320899009705\n",
            "count 24928, Loss: 0.1644260734319687\n",
            "count 24960, Loss: 0.16063711047172546\n",
            "count 24992, Loss: 0.36528846621513367\n",
            "count 25024, Loss: 0.014401006512343884\n",
            "epoch: 1\n",
            "count 32, Loss: 0.1913686841726303\n",
            "count 64, Loss: 0.12142779678106308\n",
            "count 96, Loss: 0.0726357251405716\n",
            "count 128, Loss: 0.10622034221887589\n",
            "count 160, Loss: 0.11849024146795273\n",
            "count 192, Loss: 0.3098251521587372\n",
            "count 224, Loss: 0.2586771249771118\n",
            "count 256, Loss: 0.15235869586467743\n",
            "count 288, Loss: 0.3374800682067871\n",
            "count 320, Loss: 0.09040391445159912\n",
            "count 352, Loss: 0.17254523932933807\n",
            "count 384, Loss: 0.22898808121681213\n",
            "count 416, Loss: 0.37070298194885254\n",
            "count 448, Loss: 0.16702961921691895\n",
            "count 480, Loss: 0.3204557001590729\n",
            "count 512, Loss: 0.09908872842788696\n",
            "count 544, Loss: 0.10128380358219147\n",
            "count 576, Loss: 0.0948600247502327\n",
            "count 608, Loss: 0.23658432066440582\n",
            "count 640, Loss: 0.20977269113063812\n",
            "count 672, Loss: 0.091490738093853\n",
            "count 704, Loss: 0.3920651078224182\n",
            "count 736, Loss: 0.08565020561218262\n",
            "count 768, Loss: 0.1108623594045639\n",
            "count 800, Loss: 0.10833296924829483\n",
            "count 832, Loss: 0.4230232536792755\n",
            "count 864, Loss: 0.21990203857421875\n",
            "count 896, Loss: 0.09219864755868912\n",
            "count 928, Loss: 0.289353609085083\n",
            "count 960, Loss: 0.2417173683643341\n",
            "count 992, Loss: 0.23926478624343872\n",
            "count 1024, Loss: 0.2344563901424408\n",
            "count 1056, Loss: 0.09700269997119904\n",
            "count 1088, Loss: 0.1996043622493744\n",
            "count 1120, Loss: 0.1640026569366455\n",
            "count 1152, Loss: 0.16228288412094116\n",
            "count 1184, Loss: 0.1786871999502182\n",
            "count 1216, Loss: 0.1705256700515747\n",
            "count 1248, Loss: 0.1740271896123886\n",
            "count 1280, Loss: 0.14729027450084686\n",
            "count 1312, Loss: 0.15854845941066742\n",
            "count 1344, Loss: 0.2972755432128906\n",
            "count 1376, Loss: 0.2219991385936737\n",
            "count 1408, Loss: 0.259823203086853\n",
            "count 1440, Loss: 0.09703055769205093\n",
            "count 1472, Loss: 0.07484892755746841\n",
            "count 1504, Loss: 0.10785699635744095\n",
            "count 1536, Loss: 0.25650498270988464\n",
            "count 1568, Loss: 0.10010436177253723\n",
            "count 1600, Loss: 0.07593409717082977\n",
            "count 1632, Loss: 0.4563073217868805\n",
            "count 1664, Loss: 0.1488381326198578\n",
            "count 1696, Loss: 0.19746582210063934\n",
            "count 1728, Loss: 0.1570231169462204\n",
            "count 1760, Loss: 0.15379434823989868\n",
            "count 1792, Loss: 0.3123742640018463\n",
            "count 1824, Loss: 0.15504616498947144\n",
            "count 1856, Loss: 0.11954353749752045\n",
            "count 1888, Loss: 0.47773516178131104\n",
            "count 1920, Loss: 0.1483619213104248\n",
            "count 1952, Loss: 0.3456220030784607\n",
            "count 1984, Loss: 0.03261326253414154\n",
            "count 2016, Loss: 0.0859861820936203\n",
            "count 2048, Loss: 0.18777522444725037\n",
            "count 2080, Loss: 0.20221491158008575\n",
            "count 2112, Loss: 0.1371624916791916\n",
            "count 2144, Loss: 0.2688315808773041\n",
            "count 2176, Loss: 0.25953686237335205\n",
            "count 2208, Loss: 0.27414000034332275\n",
            "count 2240, Loss: 0.14744582772254944\n",
            "count 2272, Loss: 0.17851108312606812\n",
            "count 2304, Loss: 0.16940782964229584\n",
            "count 2336, Loss: 0.12186407297849655\n",
            "count 2368, Loss: 0.12928348779678345\n",
            "count 2400, Loss: 0.22388164699077606\n",
            "count 2432, Loss: 0.11169639229774475\n",
            "count 2464, Loss: 0.2522421181201935\n",
            "count 2496, Loss: 0.1204083263874054\n",
            "count 2528, Loss: 0.28720277547836304\n",
            "count 2560, Loss: 0.30558744072914124\n",
            "count 2592, Loss: 0.17663812637329102\n",
            "count 2624, Loss: 0.197218120098114\n",
            "count 2656, Loss: 0.029227912425994873\n",
            "count 2688, Loss: 0.13486766815185547\n",
            "count 2720, Loss: 0.1682194620370865\n",
            "count 2752, Loss: 0.4188957214355469\n",
            "count 2784, Loss: 0.253503680229187\n",
            "count 2816, Loss: 0.12431932240724564\n",
            "count 2848, Loss: 0.34812840819358826\n",
            "count 2880, Loss: 0.1910373419523239\n",
            "count 2912, Loss: 0.282198041677475\n",
            "count 2944, Loss: 0.14870670437812805\n",
            "count 2976, Loss: 0.14240586757659912\n",
            "count 3008, Loss: 0.24232210218906403\n",
            "count 3040, Loss: 0.1558215469121933\n",
            "count 3072, Loss: 0.19594837725162506\n",
            "count 3104, Loss: 0.27801430225372314\n",
            "count 3136, Loss: 0.4088347554206848\n",
            "count 3168, Loss: 0.25346606969833374\n",
            "count 3200, Loss: 0.12450557947158813\n",
            "count 3232, Loss: 0.19928863644599915\n",
            "count 3264, Loss: 0.13381601870059967\n",
            "count 3296, Loss: 0.12948642671108246\n",
            "count 3328, Loss: 0.2028241604566574\n",
            "count 3360, Loss: 0.14512160420417786\n",
            "count 3392, Loss: 0.2744796872138977\n",
            "count 3424, Loss: 0.11486957967281342\n",
            "count 3456, Loss: 0.08548326790332794\n",
            "count 3488, Loss: 0.08719071000814438\n",
            "count 3520, Loss: 0.23369117081165314\n",
            "count 3552, Loss: 0.11933605372905731\n",
            "count 3584, Loss: 0.08868556469678879\n",
            "count 3616, Loss: 0.15735980868339539\n",
            "count 3648, Loss: 0.0669335201382637\n",
            "count 3680, Loss: 0.1797085851430893\n",
            "count 3712, Loss: 0.10122421383857727\n",
            "count 3744, Loss: 0.02406291291117668\n",
            "count 3776, Loss: 0.10873784869909286\n",
            "count 3808, Loss: 0.0794590413570404\n",
            "count 3840, Loss: 0.06704935431480408\n",
            "count 3872, Loss: 0.09683926403522491\n",
            "count 3904, Loss: 0.19081519544124603\n",
            "count 3936, Loss: 0.12375998497009277\n",
            "count 3968, Loss: 0.1289002001285553\n",
            "count 4000, Loss: 0.25054216384887695\n",
            "count 4032, Loss: 0.09375648200511932\n",
            "count 4064, Loss: 0.019224362447857857\n",
            "count 4096, Loss: 0.17043772339820862\n",
            "count 4128, Loss: 0.2040875256061554\n",
            "count 4160, Loss: 0.3227715790271759\n",
            "count 4192, Loss: 0.0504986047744751\n",
            "count 4224, Loss: 0.4066331088542938\n",
            "count 4256, Loss: 0.0988280326128006\n",
            "count 4288, Loss: 0.29201576113700867\n",
            "count 4320, Loss: 0.3193115293979645\n",
            "count 4352, Loss: 0.31785860657691956\n",
            "count 4384, Loss: 0.19550611078739166\n",
            "count 4416, Loss: 0.2547239661216736\n",
            "count 4448, Loss: 0.2619647979736328\n",
            "count 4480, Loss: 0.14060108363628387\n",
            "count 4512, Loss: 0.17355139553546906\n",
            "count 4544, Loss: 0.2158772349357605\n",
            "count 4576, Loss: 0.37075451016426086\n",
            "count 4608, Loss: 0.17027634382247925\n",
            "count 4640, Loss: 0.3948372006416321\n",
            "count 4672, Loss: 0.09267054498195648\n",
            "count 4704, Loss: 0.13756726682186127\n",
            "count 4736, Loss: 0.19394341111183167\n",
            "count 4768, Loss: 0.24032145738601685\n",
            "count 4800, Loss: 0.12796148657798767\n",
            "count 4832, Loss: 0.2171488106250763\n",
            "count 4864, Loss: 0.13931290805339813\n",
            "count 4896, Loss: 0.17529314756393433\n",
            "count 4928, Loss: 0.15602156519889832\n",
            "count 4960, Loss: 0.24615326523780823\n",
            "count 4992, Loss: 0.3700505197048187\n",
            "count 5024, Loss: 0.4751424789428711\n",
            "count 5056, Loss: 0.3128451406955719\n",
            "count 5088, Loss: 0.1877562552690506\n",
            "count 5120, Loss: 0.1946534812450409\n",
            "count 5152, Loss: 0.21138279139995575\n",
            "count 5184, Loss: 0.16666221618652344\n",
            "count 5216, Loss: 0.13935399055480957\n",
            "count 5248, Loss: 0.1614626944065094\n",
            "count 5280, Loss: 0.21402601897716522\n",
            "count 5312, Loss: 0.294566810131073\n",
            "count 5344, Loss: 0.2640032470226288\n",
            "count 5376, Loss: 0.19135001301765442\n",
            "count 5408, Loss: 0.0996960997581482\n",
            "count 5440, Loss: 0.1814989298582077\n",
            "count 5472, Loss: 0.04311085864901543\n",
            "count 5504, Loss: 0.28359514474868774\n",
            "count 5536, Loss: 0.3092350661754608\n",
            "count 5568, Loss: 0.1347641795873642\n",
            "count 5600, Loss: 0.1398654282093048\n",
            "count 5632, Loss: 0.23728662729263306\n",
            "count 5664, Loss: 0.20399542152881622\n",
            "count 5696, Loss: 0.20992860198020935\n",
            "count 5728, Loss: 0.2610504627227783\n",
            "count 5760, Loss: 0.19741690158843994\n",
            "count 5792, Loss: 0.06066475808620453\n",
            "count 5824, Loss: 0.1220383271574974\n",
            "count 5856, Loss: 0.14304351806640625\n",
            "count 5888, Loss: 0.19372397661209106\n",
            "count 5920, Loss: 0.08952678740024567\n",
            "count 5952, Loss: 0.2767038643360138\n",
            "count 5984, Loss: 0.1456346958875656\n",
            "count 6016, Loss: 0.21533042192459106\n",
            "count 6048, Loss: 0.1171196773648262\n",
            "count 6080, Loss: 0.22685278952121735\n",
            "count 6112, Loss: 0.19657422602176666\n",
            "count 6144, Loss: 0.21421296894550323\n",
            "count 6176, Loss: 0.08151744306087494\n",
            "count 6208, Loss: 0.041142191737890244\n",
            "count 6240, Loss: 0.14479000866413116\n",
            "count 6272, Loss: 0.21216703951358795\n",
            "count 6304, Loss: 0.14910700917243958\n",
            "count 6336, Loss: 0.16477926075458527\n",
            "count 6368, Loss: 0.2546219229698181\n",
            "count 6400, Loss: 0.10198468714952469\n",
            "count 6432, Loss: 0.19281645119190216\n",
            "count 6464, Loss: 0.07770244032144547\n",
            "count 6496, Loss: 0.20776048302650452\n",
            "count 6528, Loss: 0.22282534837722778\n",
            "count 6560, Loss: 0.14435622096061707\n",
            "count 6592, Loss: 0.2805979549884796\n",
            "count 6624, Loss: 0.37815746665000916\n",
            "count 6656, Loss: 0.361008882522583\n",
            "count 6688, Loss: 0.371570885181427\n",
            "count 6720, Loss: 0.15516529977321625\n",
            "count 6752, Loss: 0.14273163676261902\n",
            "count 6784, Loss: 0.13537892699241638\n",
            "count 6816, Loss: 0.21207381784915924\n",
            "count 6848, Loss: 0.12907923758029938\n",
            "count 6880, Loss: 0.12390299141407013\n",
            "count 6912, Loss: 0.12532161176204681\n",
            "count 6944, Loss: 0.259724885225296\n",
            "count 6976, Loss: 0.1550050973892212\n",
            "count 7008, Loss: 0.21373485028743744\n",
            "count 7040, Loss: 0.18899527192115784\n",
            "count 7072, Loss: 0.3019302189350128\n",
            "count 7104, Loss: 0.17476312816143036\n",
            "count 7136, Loss: 0.14654451608657837\n",
            "count 7168, Loss: 0.1980326771736145\n",
            "count 7200, Loss: 0.06613253802061081\n",
            "count 7232, Loss: 0.38910019397735596\n",
            "count 7264, Loss: 0.11583232134580612\n",
            "count 7296, Loss: 0.33635273575782776\n",
            "count 7328, Loss: 0.06327275931835175\n",
            "count 7360, Loss: 0.1446673423051834\n",
            "count 7392, Loss: 0.1080109030008316\n",
            "count 7424, Loss: 0.12011192739009857\n",
            "count 7456, Loss: 0.10285778343677521\n",
            "count 7488, Loss: 0.15796159207820892\n",
            "count 7520, Loss: 0.10032952576875687\n",
            "count 7552, Loss: 0.07475434988737106\n",
            "count 7584, Loss: 0.1957518607378006\n",
            "count 7616, Loss: 0.11579790711402893\n",
            "count 7648, Loss: 0.16226236522197723\n",
            "count 7680, Loss: 0.4521195888519287\n",
            "count 7712, Loss: 0.22714447975158691\n",
            "count 7744, Loss: 0.19900700449943542\n",
            "count 7776, Loss: 0.1745985895395279\n",
            "count 7808, Loss: 0.1715768575668335\n",
            "count 7840, Loss: 0.17725996673107147\n",
            "count 7872, Loss: 0.48741427063941956\n",
            "count 7904, Loss: 0.42010846734046936\n",
            "count 7936, Loss: 0.2250438630580902\n",
            "count 7968, Loss: 0.13082681596279144\n",
            "count 8000, Loss: 0.23189038038253784\n",
            "count 8032, Loss: 0.2758461534976959\n",
            "count 8064, Loss: 0.13123390078544617\n",
            "count 8096, Loss: 0.13014104962348938\n",
            "count 8128, Loss: 0.18886485695838928\n",
            "count 8160, Loss: 0.13545294106006622\n",
            "count 8192, Loss: 0.18829144537448883\n",
            "count 8224, Loss: 0.10951585322618484\n",
            "count 8256, Loss: 0.12769323587417603\n",
            "count 8288, Loss: 0.09805024415254593\n",
            "count 8320, Loss: 0.26455873250961304\n",
            "count 8352, Loss: 0.34718939661979675\n",
            "count 8384, Loss: 0.3506911098957062\n",
            "count 8416, Loss: 0.39822152256965637\n",
            "count 8448, Loss: 0.10391781479120255\n",
            "count 8480, Loss: 0.23568354547023773\n",
            "count 8512, Loss: 0.22066329419612885\n",
            "count 8544, Loss: 0.20584236085414886\n",
            "count 8576, Loss: 0.11730671674013138\n",
            "count 8608, Loss: 0.25701475143432617\n",
            "count 8640, Loss: 0.16443558037281036\n",
            "count 8672, Loss: 0.21802407503128052\n",
            "count 8704, Loss: 0.1793256402015686\n",
            "count 8736, Loss: 0.20643526315689087\n",
            "count 8768, Loss: 0.10142086446285248\n",
            "count 8800, Loss: 0.09071175009012222\n",
            "count 8832, Loss: 0.18489108979701996\n",
            "count 8864, Loss: 0.23409803211688995\n",
            "count 8896, Loss: 0.2711188495159149\n",
            "count 8928, Loss: 0.1441832035779953\n",
            "count 8960, Loss: 0.09945287555456161\n",
            "count 8992, Loss: 0.1554638296365738\n",
            "count 9024, Loss: 0.13155090808868408\n",
            "count 9056, Loss: 0.15316125750541687\n",
            "count 9088, Loss: 0.16452953219413757\n",
            "count 9120, Loss: 0.1664648950099945\n",
            "count 9152, Loss: 0.22408027946949005\n",
            "count 9184, Loss: 0.2881827652454376\n",
            "count 9216, Loss: 0.2585863769054413\n",
            "count 9248, Loss: 0.23971392214298248\n",
            "count 9280, Loss: 0.20900923013687134\n",
            "count 9312, Loss: 0.057536762207746506\n",
            "count 9344, Loss: 0.13768358528614044\n",
            "count 9376, Loss: 0.1085597574710846\n",
            "count 9408, Loss: 0.1923704296350479\n",
            "count 9440, Loss: 0.1108192428946495\n",
            "count 9472, Loss: 0.16271640360355377\n",
            "count 9504, Loss: 0.1720859259366989\n",
            "count 9536, Loss: 0.30802834033966064\n",
            "count 9568, Loss: 0.15064780414104462\n",
            "count 9600, Loss: 0.1427914798259735\n",
            "count 9632, Loss: 0.1783405989408493\n",
            "count 9664, Loss: 0.038133278489112854\n",
            "count 9696, Loss: 0.05453147366642952\n",
            "count 9728, Loss: 0.07309558242559433\n",
            "count 9760, Loss: 0.1489998698234558\n",
            "count 9792, Loss: 0.24239970743656158\n",
            "count 9824, Loss: 0.24032443761825562\n",
            "count 9856, Loss: 0.3687717914581299\n",
            "count 9888, Loss: 0.08751630038022995\n",
            "count 9920, Loss: 0.09769446402788162\n",
            "count 9952, Loss: 0.1073451042175293\n",
            "count 9984, Loss: 0.4390803575515747\n",
            "count 10016, Loss: 0.21801921725273132\n",
            "count 10048, Loss: 0.09799817949533463\n",
            "count 10080, Loss: 0.16184310615062714\n",
            "count 10112, Loss: 0.23771129548549652\n",
            "count 10144, Loss: 0.26019731163978577\n",
            "count 10176, Loss: 0.059805627912282944\n",
            "count 10208, Loss: 0.08681387454271317\n",
            "count 10240, Loss: 0.1796729862689972\n",
            "count 10272, Loss: 0.3332729935646057\n",
            "count 10304, Loss: 0.1916690468788147\n",
            "count 10336, Loss: 0.320396363735199\n",
            "count 10368, Loss: 0.11772561073303223\n",
            "count 10400, Loss: 0.07097353041172028\n",
            "count 10432, Loss: 0.39794695377349854\n",
            "count 10464, Loss: 0.15453548729419708\n",
            "count 10496, Loss: 0.08769936859607697\n",
            "count 10528, Loss: 0.2998681366443634\n",
            "count 10560, Loss: 0.46187910437583923\n",
            "count 10592, Loss: 0.30515238642692566\n",
            "count 10624, Loss: 0.21321187913417816\n",
            "count 10656, Loss: 0.08636825531721115\n",
            "count 10688, Loss: 0.14381977915763855\n",
            "count 10720, Loss: 0.17650970816612244\n",
            "count 10752, Loss: 0.21451696753501892\n",
            "count 10784, Loss: 0.28547078371047974\n",
            "count 10816, Loss: 0.1550142616033554\n",
            "count 10848, Loss: 0.144577756524086\n",
            "count 10880, Loss: 0.14595556259155273\n",
            "count 10912, Loss: 0.19323034584522247\n",
            "count 10944, Loss: 0.09598233550786972\n",
            "count 10976, Loss: 0.14421996474266052\n",
            "count 11008, Loss: 0.0935274139046669\n",
            "count 11040, Loss: 0.05341414734721184\n",
            "count 11072, Loss: 0.2756830155849457\n",
            "count 11104, Loss: 0.18394240736961365\n",
            "count 11136, Loss: 0.04306700453162193\n",
            "count 11168, Loss: 0.14737087488174438\n",
            "count 11200, Loss: 0.10786397010087967\n",
            "count 11232, Loss: 0.2245238572359085\n",
            "count 11264, Loss: 0.09631319344043732\n",
            "count 11296, Loss: 0.10920790582895279\n",
            "count 11328, Loss: 0.17990544438362122\n",
            "count 11360, Loss: 0.23408164083957672\n",
            "count 11392, Loss: 0.4419928193092346\n",
            "count 11424, Loss: 0.29637157917022705\n",
            "count 11456, Loss: 0.09528603404760361\n",
            "count 11488, Loss: 0.18737946450710297\n",
            "count 11520, Loss: 0.3162879943847656\n",
            "count 11552, Loss: 0.223673477768898\n",
            "count 11584, Loss: 0.12436791509389877\n",
            "count 11616, Loss: 0.19578832387924194\n",
            "count 11648, Loss: 0.40756988525390625\n",
            "count 11680, Loss: 0.2629261612892151\n",
            "count 11712, Loss: 0.2165619134902954\n",
            "count 11744, Loss: 0.1019492968916893\n",
            "count 11776, Loss: 0.10553997755050659\n",
            "count 11808, Loss: 0.23743748664855957\n",
            "count 11840, Loss: 0.22399701178073883\n",
            "count 11872, Loss: 0.1325242668390274\n",
            "count 11904, Loss: 0.06663485616445541\n",
            "count 11936, Loss: 0.17972683906555176\n",
            "count 11968, Loss: 0.1865781843662262\n",
            "count 12000, Loss: 0.08033078908920288\n",
            "count 12032, Loss: 0.19698388874530792\n",
            "count 12064, Loss: 0.15411357581615448\n",
            "count 12096, Loss: 0.10139525681734085\n",
            "count 12128, Loss: 0.18970263004302979\n",
            "count 12160, Loss: 0.16890564560890198\n",
            "count 12192, Loss: 0.28870534896850586\n",
            "count 12224, Loss: 0.21634799242019653\n",
            "count 12256, Loss: 0.22325602173805237\n",
            "count 12288, Loss: 0.19592081010341644\n",
            "count 12320, Loss: 0.3211100697517395\n",
            "count 12352, Loss: 0.0334508903324604\n",
            "count 12384, Loss: 0.053631071001291275\n",
            "count 12416, Loss: 0.1835518479347229\n",
            "count 12448, Loss: 0.1001201868057251\n",
            "count 12480, Loss: 0.09916906803846359\n",
            "count 12512, Loss: 0.12304945290088654\n",
            "count 12544, Loss: 0.1161578819155693\n",
            "count 12576, Loss: 0.1679583638906479\n",
            "count 12608, Loss: 0.09039206802845001\n",
            "count 12640, Loss: 0.1374615877866745\n",
            "count 12672, Loss: 0.08846152573823929\n",
            "count 12704, Loss: 0.18099170923233032\n",
            "count 12736, Loss: 0.25251227617263794\n",
            "count 12768, Loss: 0.1245737373828888\n",
            "count 12800, Loss: 0.15221668779850006\n",
            "count 12832, Loss: 0.06860198080539703\n",
            "count 12864, Loss: 0.057067155838012695\n",
            "count 12896, Loss: 0.08526530116796494\n",
            "count 12928, Loss: 0.32735559344291687\n",
            "count 12960, Loss: 0.06596243381500244\n",
            "count 12992, Loss: 0.17856205999851227\n",
            "count 13024, Loss: 0.21655014157295227\n",
            "count 13056, Loss: 0.1149136871099472\n",
            "count 13088, Loss: 0.10064344853162766\n",
            "count 13120, Loss: 0.04702190309762955\n",
            "count 13152, Loss: 0.18236209452152252\n",
            "count 13184, Loss: 0.23255854845046997\n",
            "count 13216, Loss: 0.23104819655418396\n",
            "count 13248, Loss: 0.3515293002128601\n",
            "count 13280, Loss: 0.10657242685556412\n",
            "count 13312, Loss: 0.3125144839286804\n",
            "count 13344, Loss: 0.08974616229534149\n",
            "count 13376, Loss: 0.2623334228992462\n",
            "count 13408, Loss: 0.20332978665828705\n",
            "count 13440, Loss: 0.27490657567977905\n",
            "count 13472, Loss: 0.16670109331607819\n",
            "count 13504, Loss: 0.13935737311840057\n",
            "count 13536, Loss: 0.1421939581632614\n",
            "count 13568, Loss: 0.12498384714126587\n",
            "count 13600, Loss: 0.3117958903312683\n",
            "count 13632, Loss: 0.1427127718925476\n",
            "count 13664, Loss: 0.16552551090717316\n",
            "count 13696, Loss: 0.17232482135295868\n",
            "count 13728, Loss: 0.1454772651195526\n",
            "count 13760, Loss: 0.17722387611865997\n",
            "count 13792, Loss: 0.18655015528202057\n",
            "count 13824, Loss: 0.14566612243652344\n",
            "count 13856, Loss: 0.10170567780733109\n",
            "count 13888, Loss: 0.05275528132915497\n",
            "count 13920, Loss: 0.1498517245054245\n",
            "count 13952, Loss: 0.10161443799734116\n",
            "count 13984, Loss: 0.09982996433973312\n",
            "count 14016, Loss: 0.05129550024867058\n",
            "count 14048, Loss: 0.17681309580802917\n",
            "count 14080, Loss: 0.18977411091327667\n",
            "count 14112, Loss: 0.07336334139108658\n",
            "count 14144, Loss: 0.24122342467308044\n",
            "count 14176, Loss: 0.27769455313682556\n",
            "count 14208, Loss: 0.1841389536857605\n",
            "count 14240, Loss: 0.3955630362033844\n",
            "count 14272, Loss: 0.21957571804523468\n",
            "count 14304, Loss: 0.28291475772857666\n",
            "count 14336, Loss: 0.1756773740053177\n",
            "count 14368, Loss: 0.17443373799324036\n",
            "count 14400, Loss: 0.08759217709302902\n",
            "count 14432, Loss: 0.12476451694965363\n",
            "count 14464, Loss: 0.2828094959259033\n",
            "count 14496, Loss: 0.18326522409915924\n",
            "count 14528, Loss: 0.060692597180604935\n",
            "count 14560, Loss: 0.23979732394218445\n",
            "count 14592, Loss: 0.21383140981197357\n",
            "count 14624, Loss: 0.35005438327789307\n",
            "count 14656, Loss: 0.11489184200763702\n",
            "count 14688, Loss: 0.15108904242515564\n",
            "count 14720, Loss: 0.1784110814332962\n",
            "count 14752, Loss: 0.26548072695732117\n",
            "count 14784, Loss: 0.19739508628845215\n",
            "count 14816, Loss: 0.2403717041015625\n",
            "count 14848, Loss: 0.18155553936958313\n",
            "count 14880, Loss: 0.10743669420480728\n",
            "count 14912, Loss: 0.09296761453151703\n",
            "count 14944, Loss: 0.3654272258281708\n",
            "count 14976, Loss: 0.2974106967449188\n",
            "count 15008, Loss: 0.15521879494190216\n",
            "count 15040, Loss: 0.19864381849765778\n",
            "count 15072, Loss: 0.10385459661483765\n",
            "count 15104, Loss: 0.07843690365552902\n",
            "count 15136, Loss: 0.06789132952690125\n",
            "count 15168, Loss: 0.2113652527332306\n",
            "count 15200, Loss: 0.03753973916172981\n",
            "count 15232, Loss: 0.11517002433538437\n",
            "count 15264, Loss: 0.07586216926574707\n",
            "count 15296, Loss: 0.12116595357656479\n",
            "count 15328, Loss: 0.2628699243068695\n",
            "count 15360, Loss: 0.013133098371326923\n",
            "count 15392, Loss: 0.023620963096618652\n",
            "count 15424, Loss: 0.19563518464565277\n",
            "count 15456, Loss: 0.06402282416820526\n",
            "count 15488, Loss: 0.03276190161705017\n",
            "count 15520, Loss: 0.1287083923816681\n",
            "count 15552, Loss: 0.026662496849894524\n",
            "count 15584, Loss: 0.23721924424171448\n",
            "count 15616, Loss: 0.1756073385477066\n",
            "count 15648, Loss: 0.13252654671669006\n",
            "count 15680, Loss: 0.3828957974910736\n",
            "count 15712, Loss: 0.22794075310230255\n",
            "count 15744, Loss: 0.2828752398490906\n",
            "count 15776, Loss: 0.4419214427471161\n",
            "count 15808, Loss: 0.15496434271335602\n",
            "count 15840, Loss: 0.1671430468559265\n",
            "count 15872, Loss: 0.30126968026161194\n",
            "count 15904, Loss: 0.1562366485595703\n",
            "count 15936, Loss: 0.1351226270198822\n",
            "count 15968, Loss: 0.17245922982692719\n",
            "count 16000, Loss: 0.2662399709224701\n",
            "count 16032, Loss: 0.22175303101539612\n",
            "count 16064, Loss: 0.06926938891410828\n",
            "count 16096, Loss: 0.2571212351322174\n",
            "count 16128, Loss: 0.2621617019176483\n",
            "count 16160, Loss: 0.21194234490394592\n",
            "count 16192, Loss: 0.1572471559047699\n",
            "count 16224, Loss: 0.07429546862840652\n",
            "count 16256, Loss: 0.2965783476829529\n",
            "count 16288, Loss: 0.1854662448167801\n",
            "count 16320, Loss: 0.25059765577316284\n",
            "count 16352, Loss: 0.151942178606987\n",
            "count 16384, Loss: 0.08589034527540207\n",
            "count 16416, Loss: 0.1429096758365631\n",
            "count 16448, Loss: 0.3028424382209778\n",
            "count 16480, Loss: 0.165066197514534\n",
            "count 16512, Loss: 0.14727218449115753\n",
            "count 16544, Loss: 0.44906458258628845\n",
            "count 16576, Loss: 0.1626705378293991\n",
            "count 16608, Loss: 0.21746794879436493\n",
            "count 16640, Loss: 0.1509294956922531\n",
            "count 16672, Loss: 0.12866058945655823\n",
            "count 16704, Loss: 0.18469680845737457\n",
            "count 16736, Loss: 0.08892539143562317\n",
            "count 16768, Loss: 0.22190622985363007\n",
            "count 16800, Loss: 0.13777369260787964\n",
            "count 16832, Loss: 0.19447562098503113\n",
            "count 16864, Loss: 0.09897800534963608\n",
            "count 16896, Loss: 0.03791430592536926\n",
            "count 16928, Loss: 0.06835872679948807\n",
            "count 16960, Loss: 0.1010979488492012\n",
            "count 16992, Loss: 0.10922050476074219\n",
            "count 17024, Loss: 0.1071162149310112\n",
            "count 17056, Loss: 0.028266306966543198\n",
            "count 17088, Loss: 0.4416859447956085\n",
            "count 17120, Loss: 0.14279955625534058\n",
            "count 17152, Loss: 0.05899108201265335\n",
            "count 17184, Loss: 0.27838659286499023\n",
            "count 17216, Loss: 0.1353551298379898\n",
            "count 17248, Loss: 0.2765502631664276\n",
            "count 17280, Loss: 0.4741174280643463\n",
            "count 17312, Loss: 0.20665815472602844\n",
            "count 17344, Loss: 0.12831635773181915\n",
            "count 17376, Loss: 0.4174518287181854\n",
            "count 17408, Loss: 0.1654401421546936\n",
            "count 17440, Loss: 0.050119124352931976\n",
            "count 17472, Loss: 0.11958588659763336\n",
            "count 17504, Loss: 0.19709567725658417\n",
            "count 17536, Loss: 0.14000709354877472\n",
            "count 17568, Loss: 0.1876579225063324\n",
            "count 17600, Loss: 0.28125980496406555\n",
            "count 17632, Loss: 0.20113784074783325\n",
            "count 17664, Loss: 0.30237433314323425\n",
            "count 17696, Loss: 0.32186219096183777\n",
            "count 17728, Loss: 0.19109295308589935\n",
            "count 17760, Loss: 0.13380001485347748\n",
            "count 17792, Loss: 0.14726895093917847\n",
            "count 17824, Loss: 0.2746002972126007\n",
            "count 17856, Loss: 0.2721865773200989\n",
            "count 17888, Loss: 0.0867776870727539\n",
            "count 17920, Loss: 0.2130582183599472\n",
            "count 17952, Loss: 0.2952759563922882\n",
            "count 17984, Loss: 0.28013354539871216\n",
            "count 18016, Loss: 0.17251986265182495\n",
            "count 18048, Loss: 0.09928369522094727\n",
            "count 18080, Loss: 0.2667393386363983\n",
            "count 18112, Loss: 0.1316848248243332\n",
            "count 18144, Loss: 0.35720330476760864\n",
            "count 18176, Loss: 0.0598965547978878\n",
            "count 18208, Loss: 0.15631712973117828\n",
            "count 18240, Loss: 0.16199836134910583\n",
            "count 18272, Loss: 0.024553170427680016\n",
            "count 18304, Loss: 0.09052503108978271\n",
            "count 18336, Loss: 0.07213476300239563\n",
            "count 18368, Loss: 0.11736485362052917\n",
            "count 18400, Loss: 0.10146143287420273\n",
            "count 18432, Loss: 0.13309459388256073\n",
            "count 18464, Loss: 0.1912909299135208\n",
            "count 18496, Loss: 0.37972119450569153\n",
            "count 18528, Loss: 0.15237370133399963\n",
            "count 18560, Loss: 0.19018635153770447\n",
            "count 18592, Loss: 0.39256566762924194\n",
            "count 18624, Loss: 0.07023423165082932\n",
            "count 18656, Loss: 0.15548460185527802\n",
            "count 18688, Loss: 0.10684627294540405\n",
            "count 18720, Loss: 0.1922561228275299\n",
            "count 18752, Loss: 0.29920339584350586\n",
            "count 18784, Loss: 0.23491470515727997\n",
            "count 18816, Loss: 0.4110969007015228\n",
            "count 18848, Loss: 0.12804248929023743\n",
            "count 18880, Loss: 0.24098369479179382\n",
            "count 18912, Loss: 0.25869613885879517\n",
            "count 18944, Loss: 0.05625198781490326\n",
            "count 18976, Loss: 0.1859234720468521\n",
            "count 19008, Loss: 0.22219552099704742\n",
            "count 19040, Loss: 0.06983780115842819\n",
            "count 19072, Loss: 0.2696751654148102\n",
            "count 19104, Loss: 0.11320175230503082\n",
            "count 19136, Loss: 0.12132319062948227\n",
            "count 19168, Loss: 0.16254563629627228\n",
            "count 19200, Loss: 0.15905913710594177\n",
            "count 19232, Loss: 0.1128351092338562\n",
            "count 19264, Loss: 0.3025434613227844\n",
            "count 19296, Loss: 0.17779535055160522\n",
            "count 19328, Loss: 0.17445340752601624\n",
            "count 19360, Loss: 0.24688874185085297\n",
            "count 19392, Loss: 0.13105203211307526\n",
            "count 19424, Loss: 0.2531852424144745\n",
            "count 19456, Loss: 0.2934722304344177\n",
            "count 19488, Loss: 0.4515308737754822\n",
            "count 19520, Loss: 0.28853610157966614\n",
            "count 19552, Loss: 0.27481818199157715\n",
            "count 19584, Loss: 0.09797374159097672\n",
            "count 19616, Loss: 0.17425990104675293\n",
            "count 19648, Loss: 0.15778300166130066\n",
            "count 19680, Loss: 0.19588175415992737\n",
            "count 19712, Loss: 0.1758906990289688\n",
            "count 19744, Loss: 0.25592827796936035\n",
            "count 19776, Loss: 0.08579558879137039\n",
            "count 19808, Loss: 0.2909383475780487\n",
            "count 19840, Loss: 0.06307659298181534\n",
            "count 19872, Loss: 0.2006901651620865\n",
            "count 19904, Loss: 0.10871760547161102\n",
            "count 19936, Loss: 0.17086829245090485\n",
            "count 19968, Loss: 0.2167760282754898\n",
            "count 20000, Loss: 0.11039010435342789\n",
            "count 20032, Loss: 0.13201311230659485\n",
            "count 20064, Loss: 0.6356759071350098\n",
            "count 20096, Loss: 0.20288804173469543\n",
            "count 20128, Loss: 0.186872199177742\n",
            "count 20160, Loss: 0.3668714165687561\n",
            "count 20192, Loss: 0.09829425811767578\n",
            "count 20224, Loss: 0.1537960320711136\n",
            "count 20256, Loss: 0.13351352512836456\n",
            "count 20288, Loss: 0.2618594467639923\n",
            "count 20320, Loss: 0.16263622045516968\n",
            "count 20352, Loss: 0.14556480944156647\n",
            "count 20384, Loss: 0.22964799404144287\n",
            "count 20416, Loss: 0.24072745442390442\n",
            "count 20448, Loss: 0.19100457429885864\n",
            "count 20480, Loss: 0.07428095489740372\n",
            "count 20512, Loss: 0.15266038477420807\n",
            "count 20544, Loss: 0.286062628030777\n",
            "count 20576, Loss: 0.12450625747442245\n",
            "count 20608, Loss: 0.2941027283668518\n",
            "count 20640, Loss: 0.12983189523220062\n",
            "count 20672, Loss: 0.1508949249982834\n",
            "count 20704, Loss: 0.06829030811786652\n",
            "count 20736, Loss: 0.20415247976779938\n",
            "count 20768, Loss: 0.0645328089594841\n",
            "count 20800, Loss: 0.12501777708530426\n",
            "count 20832, Loss: 0.1732700616121292\n",
            "count 20864, Loss: 0.10948631167411804\n",
            "count 20896, Loss: 0.05122573673725128\n",
            "count 20928, Loss: 0.24079950153827667\n",
            "count 20960, Loss: 0.1442597508430481\n",
            "count 20992, Loss: 0.07847905158996582\n",
            "count 21024, Loss: 0.1701820343732834\n",
            "count 21056, Loss: 0.29873573780059814\n",
            "count 21088, Loss: 0.3190586566925049\n",
            "count 21120, Loss: 0.08843552321195602\n",
            "count 21152, Loss: 0.2567221224308014\n",
            "count 21184, Loss: 0.13091997802257538\n",
            "count 21216, Loss: 0.4187634289264679\n",
            "count 21248, Loss: 0.24053628742694855\n",
            "count 21280, Loss: 0.10018813610076904\n",
            "count 21312, Loss: 0.13055282831192017\n",
            "count 21344, Loss: 0.15071237087249756\n",
            "count 21376, Loss: 0.269975483417511\n",
            "count 21408, Loss: 0.1384354680776596\n",
            "count 21440, Loss: 0.17054826021194458\n",
            "count 21472, Loss: 0.1895449310541153\n",
            "count 21504, Loss: 0.038777418434619904\n",
            "count 21536, Loss: 0.17730912566184998\n",
            "count 21568, Loss: 0.2364000827074051\n",
            "count 21600, Loss: 0.06256061792373657\n",
            "count 21632, Loss: 0.22752737998962402\n",
            "count 21664, Loss: 0.19781483709812164\n",
            "count 21696, Loss: 0.16545158624649048\n",
            "count 21728, Loss: 0.17826978862285614\n",
            "count 21760, Loss: 0.055166080594062805\n",
            "count 21792, Loss: 0.1948636919260025\n",
            "count 21824, Loss: 0.09948156774044037\n",
            "count 21856, Loss: 0.20026466250419617\n",
            "count 21888, Loss: 0.15134982764720917\n",
            "count 21920, Loss: 0.11704420298337936\n",
            "count 21952, Loss: 0.05808006599545479\n",
            "count 21984, Loss: 0.21857134997844696\n",
            "count 22016, Loss: 0.08531449735164642\n",
            "count 22048, Loss: 0.2595086693763733\n",
            "count 22080, Loss: 0.17732442915439606\n",
            "count 22112, Loss: 0.20632249116897583\n",
            "count 22144, Loss: 0.2862159013748169\n",
            "count 22176, Loss: 0.11921098828315735\n",
            "count 22208, Loss: 0.2868906557559967\n",
            "count 22240, Loss: 0.20901186764240265\n",
            "count 22272, Loss: 0.16060753166675568\n",
            "count 22304, Loss: 0.25558701157569885\n",
            "count 22336, Loss: 0.12767206132411957\n",
            "count 22368, Loss: 0.10556163638830185\n",
            "count 22400, Loss: 0.199163019657135\n",
            "count 22432, Loss: 0.246567040681839\n",
            "count 22464, Loss: 0.14587920904159546\n",
            "count 22496, Loss: 0.29063880443573\n",
            "count 22528, Loss: 0.06707467138767242\n",
            "count 22560, Loss: 0.1477554738521576\n",
            "count 22592, Loss: 0.2953362464904785\n",
            "count 22624, Loss: 0.08049862831830978\n",
            "count 22656, Loss: 0.22759677469730377\n",
            "count 22688, Loss: 0.3803882896900177\n",
            "count 22720, Loss: 0.18345287442207336\n",
            "count 22752, Loss: 0.18899686634540558\n",
            "count 22784, Loss: 0.09532710909843445\n",
            "count 22816, Loss: 0.24032188951969147\n",
            "count 22848, Loss: 0.11369971930980682\n",
            "count 22880, Loss: 0.07853186875581741\n",
            "count 22912, Loss: 0.06365106999874115\n",
            "count 22944, Loss: 0.11204449832439423\n",
            "count 22976, Loss: 0.1643519103527069\n",
            "count 23008, Loss: 0.04249981418251991\n",
            "count 23040, Loss: 0.08464030176401138\n",
            "count 23072, Loss: 0.23850829899311066\n",
            "count 23104, Loss: 0.05394142493605614\n",
            "count 23136, Loss: 0.21836771070957184\n",
            "count 23168, Loss: 0.1773742437362671\n",
            "count 23200, Loss: 0.37733274698257446\n",
            "count 23232, Loss: 0.304126501083374\n",
            "count 23264, Loss: 0.15779080986976624\n",
            "count 23296, Loss: 0.0451357401907444\n",
            "count 23328, Loss: 0.12797847390174866\n",
            "count 23360, Loss: 0.177583709359169\n",
            "count 23392, Loss: 0.11080384254455566\n",
            "count 23424, Loss: 0.13682684302330017\n",
            "count 23456, Loss: 0.5502661466598511\n",
            "count 23488, Loss: 0.1771523654460907\n",
            "count 23520, Loss: 0.14758965373039246\n",
            "count 23552, Loss: 0.12360851466655731\n",
            "count 23584, Loss: 0.3087388277053833\n",
            "count 23616, Loss: 0.21310098469257355\n",
            "count 23648, Loss: 0.05838419124484062\n",
            "count 23680, Loss: 0.26433271169662476\n",
            "count 23712, Loss: 0.20180027186870575\n",
            "count 23744, Loss: 0.20021089911460876\n",
            "count 23776, Loss: 0.11645624041557312\n",
            "count 23808, Loss: 0.09545212239027023\n",
            "count 23840, Loss: 0.390607088804245\n",
            "count 23872, Loss: 0.13734866678714752\n",
            "count 23904, Loss: 0.2245587408542633\n",
            "count 23936, Loss: 0.05752228945493698\n",
            "count 23968, Loss: 0.16816915571689606\n",
            "count 24000, Loss: 0.30511415004730225\n",
            "count 24032, Loss: 0.09973862767219543\n",
            "count 24064, Loss: 0.10052084922790527\n",
            "count 24096, Loss: 0.1794935017824173\n",
            "count 24128, Loss: 0.10723865032196045\n",
            "count 24160, Loss: 0.18472200632095337\n",
            "count 24192, Loss: 0.11311595886945724\n",
            "count 24224, Loss: 0.09097015857696533\n",
            "count 24256, Loss: 0.2059009075164795\n",
            "count 24288, Loss: 0.03233570232987404\n",
            "count 24320, Loss: 0.20594507455825806\n",
            "count 24352, Loss: 0.08674510568380356\n",
            "count 24384, Loss: 0.20127946138381958\n",
            "count 24416, Loss: 0.2560860216617584\n",
            "count 24448, Loss: 0.25359728932380676\n",
            "count 24480, Loss: 0.06520174443721771\n",
            "count 24512, Loss: 0.32854610681533813\n",
            "count 24544, Loss: 0.20307543873786926\n",
            "count 24576, Loss: 0.2287866473197937\n",
            "count 24608, Loss: 0.14040149748325348\n",
            "count 24640, Loss: 0.09335551410913467\n",
            "count 24672, Loss: 0.343463659286499\n",
            "count 24704, Loss: 0.11911867558956146\n",
            "count 24736, Loss: 0.11240909993648529\n",
            "count 24768, Loss: 0.24221673607826233\n",
            "count 24800, Loss: 0.17985716462135315\n",
            "count 24832, Loss: 0.2146701067686081\n",
            "count 24864, Loss: 0.11772001534700394\n",
            "count 24896, Loss: 0.37438690662384033\n",
            "count 24928, Loss: 0.16056303679943085\n",
            "count 24960, Loss: 0.12158896774053574\n",
            "count 24992, Loss: 0.15497912466526031\n",
            "count 25024, Loss: 0.30191585421562195\n",
            "epoch: 2\n",
            "count 32, Loss: 0.092323437333107\n",
            "count 64, Loss: 0.13765348494052887\n",
            "count 96, Loss: 0.07448703795671463\n",
            "count 128, Loss: 0.13024860620498657\n",
            "count 160, Loss: 0.21276293694972992\n",
            "count 192, Loss: 0.20942306518554688\n",
            "count 224, Loss: 0.2574700117111206\n",
            "count 256, Loss: 0.07674211263656616\n",
            "count 288, Loss: 0.17429585754871368\n",
            "count 320, Loss: 0.2380882352590561\n",
            "count 352, Loss: 0.09002266079187393\n",
            "count 384, Loss: 0.15365222096443176\n",
            "count 416, Loss: 0.11900129914283752\n",
            "count 448, Loss: 0.33796223998069763\n",
            "count 480, Loss: 0.08106918632984161\n",
            "count 512, Loss: 0.02887576073408127\n",
            "count 544, Loss: 0.21832624077796936\n",
            "count 576, Loss: 0.1252407431602478\n",
            "count 608, Loss: 0.20497363805770874\n",
            "count 640, Loss: 0.27695587277412415\n",
            "count 672, Loss: 0.1462802290916443\n",
            "count 704, Loss: 0.11509270966053009\n",
            "count 736, Loss: 0.09244771301746368\n",
            "count 768, Loss: 0.0645282119512558\n",
            "count 800, Loss: 0.13130933046340942\n",
            "count 832, Loss: 0.1501147747039795\n",
            "count 864, Loss: 0.191252201795578\n",
            "count 896, Loss: 0.10219709575176239\n",
            "count 928, Loss: 0.070212222635746\n",
            "count 960, Loss: 0.09588440507650375\n",
            "count 992, Loss: 0.046847037971019745\n",
            "count 1024, Loss: 0.2503978908061981\n",
            "count 1056, Loss: 0.10143591463565826\n",
            "count 1088, Loss: 0.18926824629306793\n",
            "count 1120, Loss: 0.03356514498591423\n",
            "count 1152, Loss: 0.03508535772562027\n",
            "count 1184, Loss: 0.08467989414930344\n",
            "count 1216, Loss: 0.03536853566765785\n",
            "count 1248, Loss: 0.01658245176076889\n",
            "count 1280, Loss: 0.03825158625841141\n",
            "count 1312, Loss: 0.016442034393548965\n",
            "count 1344, Loss: 0.028851477429270744\n",
            "count 1376, Loss: 0.17284254729747772\n",
            "count 1408, Loss: 0.13901467621326447\n",
            "count 1440, Loss: 0.05109117180109024\n",
            "count 1472, Loss: 0.07640846073627472\n",
            "count 1504, Loss: 0.08948490023612976\n",
            "count 1536, Loss: 0.03237353637814522\n",
            "count 1568, Loss: 0.10329309850931168\n",
            "count 1600, Loss: 0.2623160183429718\n",
            "count 1632, Loss: 0.051020827144384384\n",
            "count 1664, Loss: 0.17168118059635162\n",
            "count 1696, Loss: 0.037522394210100174\n",
            "count 1728, Loss: 0.2943178415298462\n",
            "count 1760, Loss: 0.05616365373134613\n",
            "count 1792, Loss: 0.11127113550901413\n",
            "count 1824, Loss: 0.1353657841682434\n",
            "count 1856, Loss: 0.10960718989372253\n",
            "count 1888, Loss: 0.21445302665233612\n",
            "count 1920, Loss: 0.23638547956943512\n",
            "count 1952, Loss: 0.24556055665016174\n",
            "count 1984, Loss: 0.02149849385023117\n",
            "count 2016, Loss: 0.2986217141151428\n",
            "count 2048, Loss: 0.21796377003192902\n",
            "count 2080, Loss: 0.09899438917636871\n",
            "count 2112, Loss: 0.031514015048742294\n",
            "count 2144, Loss: 0.060894519090652466\n",
            "count 2176, Loss: 0.07928626239299774\n",
            "count 2208, Loss: 0.05506529286503792\n",
            "count 2240, Loss: 0.09337057918310165\n",
            "count 2272, Loss: 0.06755475699901581\n",
            "count 2304, Loss: 0.05522402748465538\n",
            "count 2336, Loss: 0.11793234944343567\n",
            "count 2368, Loss: 0.12800641357898712\n",
            "count 2400, Loss: 0.1437416821718216\n",
            "count 2432, Loss: 0.09757404774427414\n",
            "count 2464, Loss: 0.04515070095658302\n",
            "count 2496, Loss: 0.2574024498462677\n",
            "count 2528, Loss: 0.055006738752126694\n",
            "count 2560, Loss: 0.18847288191318512\n",
            "count 2592, Loss: 0.17324943840503693\n",
            "count 2624, Loss: 0.07823281735181808\n",
            "count 2656, Loss: 0.055615246295928955\n",
            "count 2688, Loss: 0.3145526647567749\n",
            "count 2720, Loss: 0.13675658404827118\n",
            "count 2752, Loss: 0.24479959905147552\n",
            "count 2784, Loss: 0.1833791881799698\n",
            "count 2816, Loss: 0.024189123883843422\n",
            "count 2848, Loss: 0.15390533208847046\n",
            "count 2880, Loss: 0.1892772614955902\n",
            "count 2912, Loss: 0.07279543578624725\n",
            "count 2944, Loss: 0.3034331202507019\n",
            "count 2976, Loss: 0.23243704438209534\n",
            "count 3008, Loss: 0.09962180256843567\n",
            "count 3040, Loss: 0.1298539787530899\n",
            "count 3072, Loss: 0.10019344091415405\n",
            "count 3104, Loss: 0.258438378572464\n",
            "count 3136, Loss: 0.1718616783618927\n",
            "count 3168, Loss: 0.41604363918304443\n",
            "count 3200, Loss: 0.41622209548950195\n",
            "count 3232, Loss: 0.07274040579795837\n",
            "count 3264, Loss: 0.07108651846647263\n",
            "count 3296, Loss: 0.09293819963932037\n",
            "count 3328, Loss: 0.09255851060152054\n",
            "count 3360, Loss: 0.10273898392915726\n",
            "count 3392, Loss: 0.13892967998981476\n",
            "count 3424, Loss: 0.16690386831760406\n",
            "count 3456, Loss: 0.17348378896713257\n",
            "count 3488, Loss: 0.08900685608386993\n",
            "count 3520, Loss: 0.19808898866176605\n",
            "count 3552, Loss: 0.07104663550853729\n",
            "count 3584, Loss: 0.07545047998428345\n",
            "count 3616, Loss: 0.11637941002845764\n",
            "count 3648, Loss: 0.054809264838695526\n",
            "count 3680, Loss: 0.14545680582523346\n",
            "count 3712, Loss: 0.22488221526145935\n",
            "count 3744, Loss: 0.1833847165107727\n",
            "count 3776, Loss: 0.05822991952300072\n",
            "count 3808, Loss: 0.035764098167419434\n",
            "count 3840, Loss: 0.06650713086128235\n",
            "count 3872, Loss: 0.1965080350637436\n",
            "count 3904, Loss: 0.11705230176448822\n",
            "count 3936, Loss: 0.094202920794487\n",
            "count 3968, Loss: 0.07565058022737503\n",
            "count 4000, Loss: 0.047818005084991455\n",
            "count 4032, Loss: 0.3998641073703766\n",
            "count 4064, Loss: 0.14406119287014008\n",
            "count 4096, Loss: 0.4299653172492981\n",
            "count 4128, Loss: 0.08523643761873245\n",
            "count 4160, Loss: 0.014668331481516361\n",
            "count 4192, Loss: 0.08199853450059891\n",
            "count 4224, Loss: 0.1396980732679367\n",
            "count 4256, Loss: 0.049100156873464584\n",
            "count 4288, Loss: 0.07207431644201279\n",
            "count 4320, Loss: 0.04210445284843445\n",
            "count 4352, Loss: 0.07410678267478943\n",
            "count 4384, Loss: 0.07289306819438934\n",
            "count 4416, Loss: 0.1386130154132843\n",
            "count 4448, Loss: 0.2636307179927826\n",
            "count 4480, Loss: 0.08189485222101212\n",
            "count 4512, Loss: 0.1456139236688614\n",
            "count 4544, Loss: 0.03835584968328476\n",
            "count 4576, Loss: 0.1461280733346939\n",
            "count 4608, Loss: 0.16238214075565338\n",
            "count 4640, Loss: 0.05371573567390442\n",
            "count 4672, Loss: 0.037228088825941086\n",
            "count 4704, Loss: 0.14281490445137024\n",
            "count 4736, Loss: 0.23451820015907288\n",
            "count 4768, Loss: 0.3618681728839874\n",
            "count 4800, Loss: 0.03361344709992409\n",
            "count 4832, Loss: 0.08639182150363922\n",
            "count 4864, Loss: 0.18590714037418365\n",
            "count 4896, Loss: 0.2576623857021332\n",
            "count 4928, Loss: 0.11133293062448502\n",
            "count 4960, Loss: 0.03856447711586952\n",
            "count 4992, Loss: 0.1397112011909485\n",
            "count 5024, Loss: 0.07371728867292404\n",
            "count 5056, Loss: 0.15585175156593323\n",
            "count 5088, Loss: 0.1714063584804535\n",
            "count 5120, Loss: 0.263504296541214\n",
            "count 5152, Loss: 0.02490171790122986\n",
            "count 5184, Loss: 0.11020984500646591\n",
            "count 5216, Loss: 0.12520559132099152\n",
            "count 5248, Loss: 0.16431114077568054\n",
            "count 5280, Loss: 0.07284586876630783\n",
            "count 5312, Loss: 0.08597420901060104\n",
            "count 5344, Loss: 0.07907517999410629\n",
            "count 5376, Loss: 0.1584898978471756\n",
            "count 5408, Loss: 0.04951763153076172\n",
            "count 5440, Loss: 0.2394929975271225\n",
            "count 5472, Loss: 0.07564795762300491\n",
            "count 5504, Loss: 0.08227504789829254\n",
            "count 5536, Loss: 0.07722367346286774\n",
            "count 5568, Loss: 0.0985548198223114\n",
            "count 5600, Loss: 0.043738704174757004\n",
            "count 5632, Loss: 0.1226290687918663\n",
            "count 5664, Loss: 0.11104920506477356\n",
            "count 5696, Loss: 0.0827719122171402\n",
            "count 5728, Loss: 0.26751625537872314\n",
            "count 5760, Loss: 0.1094834953546524\n",
            "count 5792, Loss: 0.04387195035815239\n",
            "count 5824, Loss: 0.026918040588498116\n",
            "count 5856, Loss: 0.2518027424812317\n",
            "count 5888, Loss: 0.2568225562572479\n",
            "count 5920, Loss: 0.12349800765514374\n",
            "count 5952, Loss: 0.1081937626004219\n",
            "count 5984, Loss: 0.11563682556152344\n",
            "count 6016, Loss: 0.0766867995262146\n",
            "count 6048, Loss: 0.029787201434373856\n",
            "count 6080, Loss: 0.1497623175382614\n",
            "count 6112, Loss: 0.039436064660549164\n",
            "count 6144, Loss: 0.0838971957564354\n",
            "count 6176, Loss: 0.03698078915476799\n",
            "count 6208, Loss: 0.1688748449087143\n",
            "count 6240, Loss: 0.04328480735421181\n",
            "count 6272, Loss: 0.08375688642263412\n",
            "count 6304, Loss: 0.1638108789920807\n",
            "count 6336, Loss: 0.048978112637996674\n",
            "count 6368, Loss: 0.02660325914621353\n",
            "count 6400, Loss: 0.03329088166356087\n",
            "count 6432, Loss: 0.03155113756656647\n",
            "count 6464, Loss: 0.11186172813177109\n",
            "count 6496, Loss: 0.08635442703962326\n",
            "count 6528, Loss: 0.011461996473371983\n",
            "count 6560, Loss: 0.05992140620946884\n",
            "count 6592, Loss: 0.049490731209516525\n",
            "count 6624, Loss: 0.0786898210644722\n",
            "count 6656, Loss: 0.006249122321605682\n",
            "count 6688, Loss: 0.25056615471839905\n",
            "count 6720, Loss: 0.02238495834171772\n",
            "count 6752, Loss: 0.09099657088518143\n",
            "count 6784, Loss: 0.09491533041000366\n",
            "count 6816, Loss: 0.04194624722003937\n",
            "count 6848, Loss: 0.20398743450641632\n",
            "count 6880, Loss: 0.07190830260515213\n",
            "count 6912, Loss: 0.04171115159988403\n",
            "count 6944, Loss: 0.11954668909311295\n",
            "count 6976, Loss: 0.050133295357227325\n",
            "count 7008, Loss: 0.1358233392238617\n",
            "count 7040, Loss: 0.018029076978564262\n",
            "count 7072, Loss: 0.06603547185659409\n",
            "count 7104, Loss: 0.059907466173172\n",
            "count 7136, Loss: 0.08720751851797104\n",
            "count 7168, Loss: 0.07699866592884064\n",
            "count 7200, Loss: 0.16734358668327332\n",
            "count 7232, Loss: 0.03441564366221428\n",
            "count 7264, Loss: 0.0687091052532196\n",
            "count 7296, Loss: 0.2903617322444916\n",
            "count 7328, Loss: 0.16789749264717102\n",
            "count 7360, Loss: 0.09050007164478302\n",
            "count 7392, Loss: 0.09694229811429977\n",
            "count 7424, Loss: 0.01098403986543417\n",
            "count 7456, Loss: 0.009135683067142963\n",
            "count 7488, Loss: 0.14724110066890717\n",
            "count 7520, Loss: 0.0585806742310524\n",
            "count 7552, Loss: 0.04249582812190056\n",
            "count 7584, Loss: 0.1312197595834732\n",
            "count 7616, Loss: 0.016191374510526657\n",
            "count 7648, Loss: 0.15623319149017334\n",
            "count 7680, Loss: 0.155922532081604\n",
            "count 7712, Loss: 0.229887455701828\n",
            "count 7744, Loss: 0.08301693201065063\n",
            "count 7776, Loss: 0.10602863878011703\n",
            "count 7808, Loss: 0.04584712162613869\n",
            "count 7840, Loss: 0.14297519624233246\n",
            "count 7872, Loss: 0.22500558197498322\n",
            "count 7904, Loss: 0.02888484299182892\n",
            "count 7936, Loss: 0.06062914431095123\n",
            "count 7968, Loss: 0.07715673744678497\n",
            "count 8000, Loss: 0.08356413245201111\n",
            "count 8032, Loss: 0.02467058226466179\n",
            "count 8064, Loss: 0.10386720299720764\n",
            "count 8096, Loss: 0.2612770199775696\n",
            "count 8128, Loss: 0.10539790987968445\n",
            "count 8160, Loss: 0.08475825935602188\n",
            "count 8192, Loss: 0.09754665195941925\n",
            "count 8224, Loss: 0.10898111015558243\n",
            "count 8256, Loss: 0.41138648986816406\n",
            "count 8288, Loss: 0.056191910058259964\n",
            "count 8320, Loss: 0.06882815062999725\n",
            "count 8352, Loss: 0.09402245283126831\n",
            "count 8384, Loss: 0.05845823138952255\n",
            "count 8416, Loss: 0.04768703505396843\n",
            "count 8448, Loss: 0.09130693227052689\n",
            "count 8480, Loss: 0.19415166974067688\n",
            "count 8512, Loss: 0.03557193651795387\n",
            "count 8544, Loss: 0.17963533103466034\n",
            "count 8576, Loss: 0.12099642306566238\n",
            "count 8608, Loss: 0.1941552609205246\n",
            "count 8640, Loss: 0.1350518763065338\n",
            "count 8672, Loss: 0.07855270057916641\n",
            "count 8704, Loss: 0.020232822746038437\n",
            "count 8736, Loss: 0.17094628512859344\n",
            "count 8768, Loss: 0.1690676361322403\n",
            "count 8800, Loss: 0.13137713074684143\n",
            "count 8832, Loss: 0.027883082628250122\n",
            "count 8864, Loss: 0.02515804022550583\n",
            "count 8896, Loss: 0.26883238554000854\n",
            "count 8928, Loss: 0.13807062804698944\n",
            "count 8960, Loss: 0.09878762811422348\n",
            "count 8992, Loss: 0.10503854602575302\n",
            "count 9024, Loss: 0.24749843776226044\n",
            "count 9056, Loss: 0.11137386411428452\n",
            "count 9088, Loss: 0.0357239656150341\n",
            "count 9120, Loss: 0.07035598903894424\n",
            "count 9152, Loss: 0.05491342023015022\n",
            "count 9184, Loss: 0.155352383852005\n",
            "count 9216, Loss: 0.21492376923561096\n",
            "count 9248, Loss: 0.07141323387622833\n",
            "count 9280, Loss: 0.20552965998649597\n",
            "count 9312, Loss: 0.042153626680374146\n",
            "count 9344, Loss: 0.0500757060945034\n",
            "count 9376, Loss: 0.23503023386001587\n",
            "count 9408, Loss: 0.2052845060825348\n",
            "count 9440, Loss: 0.028469795361161232\n",
            "count 9472, Loss: 0.14469479024410248\n",
            "count 9504, Loss: 0.24388644099235535\n",
            "count 9536, Loss: 0.03604704886674881\n",
            "count 9568, Loss: 0.3430764377117157\n",
            "count 9600, Loss: 0.05596212297677994\n",
            "count 9632, Loss: 0.07121696323156357\n",
            "count 9664, Loss: 0.281048983335495\n",
            "count 9696, Loss: 0.010920503176748753\n",
            "count 9728, Loss: 0.16741688549518585\n",
            "count 9760, Loss: 0.11774598062038422\n",
            "count 9792, Loss: 0.22966086864471436\n",
            "count 9824, Loss: 0.11006758362054825\n",
            "count 9856, Loss: 0.21969938278198242\n",
            "count 9888, Loss: 0.3930772840976715\n",
            "count 9920, Loss: 0.22395625710487366\n",
            "count 9952, Loss: 0.14561137557029724\n",
            "count 9984, Loss: 0.09621401131153107\n",
            "count 10016, Loss: 0.05011367425322533\n",
            "count 10048, Loss: 0.20843134820461273\n",
            "count 10080, Loss: 0.14672207832336426\n",
            "count 10112, Loss: 0.0611606240272522\n",
            "count 10144, Loss: 0.1541157215833664\n",
            "count 10176, Loss: 0.06949827075004578\n",
            "count 10208, Loss: 0.22790071368217468\n",
            "count 10240, Loss: 0.11058837175369263\n",
            "count 10272, Loss: 0.09896820038557053\n",
            "count 10304, Loss: 0.03764761984348297\n",
            "count 10336, Loss: 0.044910550117492676\n",
            "count 10368, Loss: 0.10293958336114883\n",
            "count 10400, Loss: 0.19973264634609222\n",
            "count 10432, Loss: 0.2347135841846466\n",
            "count 10464, Loss: 0.17624425888061523\n",
            "count 10496, Loss: 0.1609024554491043\n",
            "count 10528, Loss: 0.0908844918012619\n",
            "count 10560, Loss: 0.21114741265773773\n",
            "count 10592, Loss: 0.31258341670036316\n",
            "count 10624, Loss: 0.09518223255872726\n",
            "count 10656, Loss: 0.26975664496421814\n",
            "count 10688, Loss: 0.16854585707187653\n",
            "count 10720, Loss: 0.16041196882724762\n",
            "count 10752, Loss: 0.26594987511634827\n",
            "count 10784, Loss: 0.07557782530784607\n",
            "count 10816, Loss: 0.20493420958518982\n",
            "count 10848, Loss: 0.2480362355709076\n",
            "count 10880, Loss: 0.24428191781044006\n",
            "count 10912, Loss: 0.3423805832862854\n",
            "count 10944, Loss: 0.15227803587913513\n",
            "count 10976, Loss: 0.08537080883979797\n",
            "count 11008, Loss: 0.07056338340044022\n",
            "count 11040, Loss: 0.08752364665269852\n",
            "count 11072, Loss: 0.12456464022397995\n",
            "count 11104, Loss: 0.18295158445835114\n",
            "count 11136, Loss: 0.18771499395370483\n",
            "count 11168, Loss: 0.16069374978542328\n",
            "count 11200, Loss: 0.09008338302373886\n",
            "count 11232, Loss: 0.03144402801990509\n",
            "count 11264, Loss: 0.04990247264504433\n",
            "count 11296, Loss: 0.14763416349887848\n",
            "count 11328, Loss: 0.10839799791574478\n",
            "count 11360, Loss: 0.13386650383472443\n",
            "count 11392, Loss: 0.08191432803869247\n",
            "count 11424, Loss: 0.19432470202445984\n",
            "count 11456, Loss: 0.06879974156618118\n",
            "count 11488, Loss: 0.12110243737697601\n",
            "count 11520, Loss: 0.09394413977861404\n",
            "count 11552, Loss: 0.07961579412221909\n",
            "count 11584, Loss: 0.05581904202699661\n",
            "count 11616, Loss: 0.1647140383720398\n",
            "count 11648, Loss: 0.10217230767011642\n",
            "count 11680, Loss: 0.04182141274213791\n",
            "count 11712, Loss: 0.045132752507925034\n",
            "count 11744, Loss: 0.0860287994146347\n",
            "count 11776, Loss: 0.0576971210539341\n",
            "count 11808, Loss: 0.11149385571479797\n",
            "count 11840, Loss: 0.009622115641832352\n",
            "count 11872, Loss: 0.02516845613718033\n",
            "count 11904, Loss: 0.11897535622119904\n",
            "count 11936, Loss: 0.2939515709877014\n",
            "count 11968, Loss: 0.22979554533958435\n",
            "count 12000, Loss: 0.11150199919939041\n",
            "count 12032, Loss: 0.1623084992170334\n",
            "count 12064, Loss: 0.22520285844802856\n",
            "count 12096, Loss: 0.10263994336128235\n",
            "count 12128, Loss: 0.18082815408706665\n",
            "count 12160, Loss: 0.3647037148475647\n",
            "count 12192, Loss: 0.11892076581716537\n",
            "count 12224, Loss: 0.0362701341509819\n",
            "count 12256, Loss: 0.142540842294693\n",
            "count 12288, Loss: 0.10069277137517929\n",
            "count 12320, Loss: 0.05799344182014465\n",
            "count 12352, Loss: 0.06887597590684891\n",
            "count 12384, Loss: 0.120201475918293\n",
            "count 12416, Loss: 0.046757787466049194\n",
            "count 12448, Loss: 0.14277005195617676\n",
            "count 12480, Loss: 0.12254303693771362\n",
            "count 12512, Loss: 0.12028346210718155\n",
            "count 12544, Loss: 0.049126867204904556\n",
            "count 12576, Loss: 0.2100708931684494\n",
            "count 12608, Loss: 0.01749054156243801\n",
            "count 12640, Loss: 0.04232344403862953\n",
            "count 12672, Loss: 0.07017932087182999\n",
            "count 12704, Loss: 0.10608553141355515\n",
            "count 12736, Loss: 0.28143587708473206\n",
            "count 12768, Loss: 0.016112886369228363\n",
            "count 12800, Loss: 0.1274922639131546\n",
            "count 12832, Loss: 0.081693634390831\n",
            "count 12864, Loss: 0.3944562077522278\n",
            "count 12896, Loss: 0.006701228208839893\n",
            "count 12928, Loss: 0.1951131522655487\n",
            "count 12960, Loss: 0.23185035586357117\n",
            "count 12992, Loss: 0.03784329816699028\n",
            "count 13024, Loss: 0.17618951201438904\n",
            "count 13056, Loss: 0.04857422783970833\n",
            "count 13088, Loss: 0.2027912735939026\n",
            "count 13120, Loss: 0.1336309015750885\n",
            "count 13152, Loss: 0.012383527122437954\n",
            "count 13184, Loss: 0.07354355603456497\n",
            "count 13216, Loss: 0.3051360845565796\n",
            "count 13248, Loss: 0.3524949550628662\n",
            "count 13280, Loss: 0.18599320948123932\n",
            "count 13312, Loss: 0.44765704870224\n",
            "count 13344, Loss: 0.09030140936374664\n",
            "count 13376, Loss: 0.05780306085944176\n",
            "count 13408, Loss: 0.3222905099391937\n",
            "count 13440, Loss: 0.04136049747467041\n",
            "count 13472, Loss: 0.14229318499565125\n",
            "count 13504, Loss: 0.21442487835884094\n",
            "count 13536, Loss: 0.06219559535384178\n",
            "count 13568, Loss: 0.08750664442777634\n",
            "count 13600, Loss: 0.08064636588096619\n",
            "count 13632, Loss: 0.18169070780277252\n",
            "count 13664, Loss: 0.34178945422172546\n",
            "count 13696, Loss: 0.17420335114002228\n",
            "count 13728, Loss: 0.08083228021860123\n",
            "count 13760, Loss: 0.08290571719408035\n",
            "count 13792, Loss: 0.07435526698827744\n",
            "count 13824, Loss: 0.2318166047334671\n",
            "count 13856, Loss: 0.14310219883918762\n",
            "count 13888, Loss: 0.22650618851184845\n",
            "count 13920, Loss: 0.0823274478316307\n",
            "count 13952, Loss: 0.042090509086847305\n",
            "count 13984, Loss: 0.06645592302083969\n",
            "count 14016, Loss: 0.027904190123081207\n",
            "count 14048, Loss: 0.06348340213298798\n",
            "count 14080, Loss: 0.03834998607635498\n",
            "count 14112, Loss: 0.31245848536491394\n",
            "count 14144, Loss: 0.2406417280435562\n",
            "count 14176, Loss: 0.23091237246990204\n",
            "count 14208, Loss: 0.28852370381355286\n",
            "count 14240, Loss: 0.03242965042591095\n",
            "count 14272, Loss: 0.043528031557798386\n",
            "count 14304, Loss: 0.11080964654684067\n",
            "count 14336, Loss: 0.039785683155059814\n",
            "count 14368, Loss: 0.27696433663368225\n",
            "count 14400, Loss: 0.2750595510005951\n",
            "count 14432, Loss: 0.20732654631137848\n",
            "count 14464, Loss: 0.0723593533039093\n",
            "count 14496, Loss: 0.28965210914611816\n",
            "count 14528, Loss: 0.1250976324081421\n",
            "count 14560, Loss: 0.2547226846218109\n",
            "count 14592, Loss: 0.09272660315036774\n",
            "count 14624, Loss: 0.284440279006958\n",
            "count 14656, Loss: 0.11390344798564911\n",
            "count 14688, Loss: 0.11660867929458618\n",
            "count 14720, Loss: 0.020001625642180443\n",
            "count 14752, Loss: 0.18546459078788757\n",
            "count 14784, Loss: 0.09383687376976013\n",
            "count 14816, Loss: 0.07057439535856247\n",
            "count 14848, Loss: 0.11078344285488129\n",
            "count 14880, Loss: 0.05868283659219742\n",
            "count 14912, Loss: 0.1003270298242569\n",
            "count 14944, Loss: 0.030643107369542122\n",
            "count 14976, Loss: 0.047285471111536026\n",
            "count 15008, Loss: 0.1331631988286972\n",
            "count 15040, Loss: 0.2628166973590851\n",
            "count 15072, Loss: 0.033918336033821106\n",
            "count 15104, Loss: 0.057748325169086456\n",
            "count 15136, Loss: 0.4649617373943329\n",
            "count 15168, Loss: 0.05213894322514534\n",
            "count 15200, Loss: 0.16409634053707123\n",
            "count 15232, Loss: 0.05997024476528168\n",
            "count 15264, Loss: 0.1887325793504715\n",
            "count 15296, Loss: 0.13488860428333282\n",
            "count 15328, Loss: 0.036569174379110336\n",
            "count 15360, Loss: 0.19100046157836914\n",
            "count 15392, Loss: 0.12631714344024658\n",
            "count 15424, Loss: 0.0292705986648798\n",
            "count 15456, Loss: 0.1383240669965744\n",
            "count 15488, Loss: 0.20093293488025665\n",
            "count 15520, Loss: 0.08854103088378906\n",
            "count 15552, Loss: 0.25880372524261475\n",
            "count 15584, Loss: 0.03220546245574951\n",
            "count 15616, Loss: 0.05257613956928253\n",
            "count 15648, Loss: 0.025001781061291695\n",
            "count 15680, Loss: 0.0895623117685318\n",
            "count 15712, Loss: 0.06547965854406357\n",
            "count 15744, Loss: 0.13017773628234863\n",
            "count 15776, Loss: 0.05912373214960098\n",
            "count 15808, Loss: 0.0681673213839531\n",
            "count 15840, Loss: 0.08742356300354004\n",
            "count 15872, Loss: 0.18296192586421967\n",
            "count 15904, Loss: 0.020107759162783623\n",
            "count 15936, Loss: 0.19188691675662994\n",
            "count 15968, Loss: 0.3487035632133484\n",
            "count 16000, Loss: 0.20197255909442902\n",
            "count 16032, Loss: 0.11472734808921814\n",
            "count 16064, Loss: 0.4172036349773407\n",
            "count 16096, Loss: 0.12445259094238281\n",
            "count 16128, Loss: 0.1687740534543991\n",
            "count 16160, Loss: 0.3067956268787384\n",
            "count 16192, Loss: 0.0402071550488472\n",
            "count 16224, Loss: 0.06975939124822617\n",
            "count 16256, Loss: 0.09405876696109772\n",
            "count 16288, Loss: 0.06137499213218689\n",
            "count 16320, Loss: 0.10530290007591248\n",
            "count 16352, Loss: 0.3301100432872772\n",
            "count 16384, Loss: 0.07597081363201141\n",
            "count 16416, Loss: 0.1955784559249878\n",
            "count 16448, Loss: 0.1253301054239273\n",
            "count 16480, Loss: 0.21825717389583588\n",
            "count 16512, Loss: 0.1071331650018692\n",
            "count 16544, Loss: 0.12470322847366333\n",
            "count 16576, Loss: 0.2555883526802063\n",
            "count 16608, Loss: 0.13520129024982452\n",
            "count 16640, Loss: 0.02615288272500038\n",
            "count 16672, Loss: 0.1998845338821411\n",
            "count 16704, Loss: 0.07982290536165237\n",
            "count 16736, Loss: 0.12708427011966705\n",
            "count 16768, Loss: 0.1975664645433426\n",
            "count 16800, Loss: 0.05526665598154068\n",
            "count 16832, Loss: 0.09201760590076447\n",
            "count 16864, Loss: 0.2102208286523819\n",
            "count 16896, Loss: 0.2866082191467285\n",
            "count 16928, Loss: 0.09688963741064072\n",
            "count 16960, Loss: 0.02590678632259369\n",
            "count 16992, Loss: 0.07661385089159012\n",
            "count 17024, Loss: 0.05555623024702072\n",
            "count 17056, Loss: 0.0767538845539093\n",
            "count 17088, Loss: 0.030891835689544678\n",
            "count 17120, Loss: 0.06159454956650734\n",
            "count 17152, Loss: 0.19846783578395844\n",
            "count 17184, Loss: 0.0882338285446167\n",
            "count 17216, Loss: 0.17373667657375336\n",
            "count 17248, Loss: 0.03228561580181122\n",
            "count 17280, Loss: 0.07509453594684601\n",
            "count 17312, Loss: 0.06068932265043259\n",
            "count 17344, Loss: 0.11398286372423172\n",
            "count 17376, Loss: 0.03612698242068291\n",
            "count 17408, Loss: 0.0103461854159832\n",
            "count 17440, Loss: 0.02471572905778885\n",
            "count 17472, Loss: 0.033090557903051376\n",
            "count 17504, Loss: 0.04610350728034973\n",
            "count 17536, Loss: 0.16685453057289124\n",
            "count 17568, Loss: 0.16729441285133362\n",
            "count 17600, Loss: 0.30903974175453186\n",
            "count 17632, Loss: 0.03261500597000122\n",
            "count 17664, Loss: 0.15626105666160583\n",
            "count 17696, Loss: 0.05821438506245613\n",
            "count 17728, Loss: 0.06780504435300827\n",
            "count 17760, Loss: 0.04485755041241646\n",
            "count 17792, Loss: 0.1352623850107193\n",
            "count 17824, Loss: 0.1030631810426712\n",
            "count 17856, Loss: 0.08101671189069748\n",
            "count 17888, Loss: 0.06615062803030014\n",
            "count 17920, Loss: 0.09759663790464401\n",
            "count 17952, Loss: 0.15697288513183594\n",
            "count 17984, Loss: 0.1299431174993515\n",
            "count 18016, Loss: 0.16097262501716614\n",
            "count 18048, Loss: 0.11251842230558395\n",
            "count 18080, Loss: 0.03922538086771965\n",
            "count 18112, Loss: 0.09144023805856705\n",
            "count 18144, Loss: 0.24836204946041107\n",
            "count 18176, Loss: 0.4656215310096741\n",
            "count 18208, Loss: 0.20263251662254333\n",
            "count 18240, Loss: 0.11522931605577469\n",
            "count 18272, Loss: 0.10368705540895462\n",
            "count 18304, Loss: 0.04587965086102486\n",
            "count 18336, Loss: 0.520816445350647\n",
            "count 18368, Loss: 0.1782166212797165\n",
            "count 18400, Loss: 0.07621002197265625\n",
            "count 18432, Loss: 0.1825553923845291\n",
            "count 18464, Loss: 0.15130510926246643\n",
            "count 18496, Loss: 0.05998881533741951\n",
            "count 18528, Loss: 0.264263391494751\n",
            "count 18560, Loss: 0.14963658154010773\n",
            "count 18592, Loss: 0.20404459536075592\n",
            "count 18624, Loss: 0.3494418263435364\n",
            "count 18656, Loss: 0.1957545429468155\n",
            "count 18688, Loss: 0.0780341699719429\n",
            "count 18720, Loss: 0.10921663045883179\n",
            "count 18752, Loss: 0.16002026200294495\n",
            "count 18784, Loss: 0.25782454013824463\n",
            "count 18816, Loss: 0.21103139221668243\n",
            "count 18848, Loss: 0.13313625752925873\n",
            "count 18880, Loss: 0.12831097841262817\n",
            "count 18912, Loss: 0.14246727526187897\n",
            "count 18944, Loss: 0.1184563934803009\n",
            "count 18976, Loss: 0.09694480895996094\n",
            "count 19008, Loss: 0.23611098527908325\n",
            "count 19040, Loss: 0.23738814890384674\n",
            "count 19072, Loss: 0.07032559812068939\n",
            "count 19104, Loss: 0.2516283094882965\n",
            "count 19136, Loss: 0.1415543258190155\n",
            "count 19168, Loss: 0.0324610099196434\n",
            "count 19200, Loss: 0.03440479189157486\n",
            "count 19232, Loss: 0.13917826116085052\n",
            "count 19264, Loss: 0.08584772795438766\n",
            "count 19296, Loss: 0.24751365184783936\n",
            "count 19328, Loss: 0.24853453040122986\n",
            "count 19360, Loss: 0.08010115474462509\n",
            "count 19392, Loss: 0.159833624958992\n",
            "count 19424, Loss: 0.08727169036865234\n",
            "count 19456, Loss: 0.0692472755908966\n",
            "count 19488, Loss: 0.11052750796079636\n",
            "count 19520, Loss: 0.1342252641916275\n",
            "count 19552, Loss: 0.02549889124929905\n",
            "count 19584, Loss: 0.09460709244012833\n",
            "count 19616, Loss: 0.06932800263166428\n",
            "count 19648, Loss: 0.20556354522705078\n",
            "count 19680, Loss: 0.10248361527919769\n",
            "count 19712, Loss: 0.07786580920219421\n",
            "count 19744, Loss: 0.05567852407693863\n",
            "count 19776, Loss: 0.0681496188044548\n",
            "count 19808, Loss: 0.26040929555892944\n",
            "count 19840, Loss: 0.05146796256303787\n",
            "count 19872, Loss: 0.2155943065881729\n",
            "count 19904, Loss: 0.2838204801082611\n",
            "count 19936, Loss: 0.025416620075702667\n",
            "count 19968, Loss: 0.06106099113821983\n",
            "count 20000, Loss: 0.040239833295345306\n",
            "count 20032, Loss: 0.11882380396127701\n",
            "count 20064, Loss: 0.05694945901632309\n",
            "count 20096, Loss: 0.03195551410317421\n",
            "count 20128, Loss: 0.20300817489624023\n",
            "count 20160, Loss: 0.09854141622781754\n",
            "count 20192, Loss: 0.02984749898314476\n",
            "count 20224, Loss: 0.45293128490448\n",
            "count 20256, Loss: 0.04640483111143112\n",
            "count 20288, Loss: 0.17514795064926147\n",
            "count 20320, Loss: 0.05183780565857887\n",
            "count 20352, Loss: 0.029982779175043106\n",
            "count 20384, Loss: 0.305727481842041\n",
            "count 20416, Loss: 0.12043546140193939\n",
            "count 20448, Loss: 0.21279847621917725\n",
            "count 20480, Loss: 0.09835968166589737\n",
            "count 20512, Loss: 0.15658429265022278\n",
            "count 20544, Loss: 0.050555817782878876\n",
            "count 20576, Loss: 0.030090268701314926\n",
            "count 20608, Loss: 0.1526709794998169\n",
            "count 20640, Loss: 0.16393183171749115\n",
            "count 20672, Loss: 0.023235199972987175\n",
            "count 20704, Loss: 0.022229749709367752\n",
            "count 20736, Loss: 0.01821744814515114\n",
            "count 20768, Loss: 0.12066087871789932\n",
            "count 20800, Loss: 0.13756023347377777\n",
            "count 20832, Loss: 0.10558871179819107\n",
            "count 20864, Loss: 0.0357300229370594\n",
            "count 20896, Loss: 0.21733707189559937\n",
            "count 20928, Loss: 0.06267298012971878\n",
            "count 20960, Loss: 0.0776679664850235\n",
            "count 20992, Loss: 0.09397891908884048\n",
            "count 21024, Loss: 0.18626122176647186\n",
            "count 21056, Loss: 0.2972058057785034\n",
            "count 21088, Loss: 0.11100999265909195\n",
            "count 21120, Loss: 0.0929805263876915\n",
            "count 21152, Loss: 0.032046761363744736\n",
            "count 21184, Loss: 0.03925028070807457\n",
            "count 21216, Loss: 0.11952347308397293\n",
            "count 21248, Loss: 0.17904701828956604\n",
            "count 21280, Loss: 0.19884009659290314\n",
            "count 21312, Loss: 0.4981691539287567\n",
            "count 21344, Loss: 0.13284768164157867\n",
            "count 21376, Loss: 0.19901810586452484\n",
            "count 21408, Loss: 0.20388348400592804\n",
            "count 21440, Loss: 0.2279365211725235\n",
            "count 21472, Loss: 0.014707162044942379\n",
            "count 21504, Loss: 0.21438457071781158\n",
            "count 21536, Loss: 0.1057359054684639\n",
            "count 21568, Loss: 0.19595541059970856\n",
            "count 21600, Loss: 0.146236389875412\n",
            "count 21632, Loss: 0.24261346459388733\n",
            "count 21664, Loss: 0.07164578139781952\n",
            "count 21696, Loss: 0.062203481793403625\n",
            "count 21728, Loss: 0.035046614706516266\n",
            "count 21760, Loss: 0.1689964383840561\n",
            "count 21792, Loss: 0.06424854695796967\n",
            "count 21824, Loss: 0.12025502324104309\n",
            "count 21856, Loss: 0.22588542103767395\n",
            "count 21888, Loss: 0.21531811356544495\n",
            "count 21920, Loss: 0.06852614879608154\n",
            "count 21952, Loss: 0.08508261293172836\n",
            "count 21984, Loss: 0.23818457126617432\n",
            "count 22016, Loss: 0.21338269114494324\n",
            "count 22048, Loss: 0.09024247527122498\n",
            "count 22080, Loss: 0.37589919567108154\n",
            "count 22112, Loss: 0.032377131283283234\n",
            "count 22144, Loss: 0.019260097295045853\n",
            "count 22176, Loss: 0.05586869269609451\n",
            "count 22208, Loss: 0.10647939145565033\n",
            "count 22240, Loss: 0.1647288203239441\n",
            "count 22272, Loss: 0.29568296670913696\n",
            "count 22304, Loss: 0.11125094443559647\n",
            "count 22336, Loss: 0.12345638871192932\n",
            "count 22368, Loss: 0.18313051760196686\n",
            "count 22400, Loss: 0.28814756870269775\n",
            "count 22432, Loss: 0.21696113049983978\n",
            "count 22464, Loss: 0.11957422643899918\n",
            "count 22496, Loss: 0.36361047625541687\n",
            "count 22528, Loss: 0.09301964938640594\n",
            "count 22560, Loss: 0.08812519907951355\n",
            "count 22592, Loss: 0.033416688442230225\n",
            "count 22624, Loss: 0.06874942034482956\n",
            "count 22656, Loss: 0.08008112013339996\n",
            "count 22688, Loss: 0.08253941684961319\n",
            "count 22720, Loss: 0.03918449580669403\n",
            "count 22752, Loss: 0.20731718838214874\n",
            "count 22784, Loss: 0.18080981075763702\n",
            "count 22816, Loss: 0.04925689846277237\n",
            "count 22848, Loss: 0.10990636050701141\n",
            "count 22880, Loss: 0.10593768954277039\n",
            "count 22912, Loss: 0.10754790902137756\n",
            "count 22944, Loss: 0.05944124236702919\n",
            "count 22976, Loss: 0.08133071660995483\n",
            "count 23008, Loss: 0.10117192566394806\n",
            "count 23040, Loss: 0.04984939098358154\n",
            "count 23072, Loss: 0.24351274967193604\n",
            "count 23104, Loss: 0.04640980809926987\n",
            "count 23136, Loss: 0.1115327700972557\n",
            "count 23168, Loss: 0.08740363270044327\n",
            "count 23200, Loss: 0.06384876370429993\n",
            "count 23232, Loss: 0.052432917058467865\n",
            "count 23264, Loss: 0.08340533077716827\n",
            "count 23296, Loss: 0.1527668833732605\n",
            "count 23328, Loss: 0.1624733805656433\n",
            "count 23360, Loss: 0.05863520875573158\n",
            "count 23392, Loss: 0.08570503443479538\n",
            "count 23424, Loss: 0.036915373057127\n",
            "count 23456, Loss: 0.06615453958511353\n",
            "count 23488, Loss: 0.04247669503092766\n",
            "count 23520, Loss: 0.1270817518234253\n",
            "count 23552, Loss: 0.050106730312108994\n",
            "count 23584, Loss: 0.3654383718967438\n",
            "count 23616, Loss: 0.021054008975625038\n",
            "count 23648, Loss: 0.2483852505683899\n",
            "count 23680, Loss: 0.15556088089942932\n",
            "count 23712, Loss: 0.009767313487827778\n",
            "count 23744, Loss: 0.046093303710222244\n",
            "count 23776, Loss: 0.2317810207605362\n",
            "count 23808, Loss: 0.017658796161413193\n",
            "count 23840, Loss: 0.16913510859012604\n",
            "count 23872, Loss: 0.16052813827991486\n",
            "count 23904, Loss: 0.23676326870918274\n",
            "count 23936, Loss: 0.11248890310525894\n",
            "count 23968, Loss: 0.3090560734272003\n",
            "count 24000, Loss: 0.0656621903181076\n",
            "count 24032, Loss: 0.12313603609800339\n",
            "count 24064, Loss: 0.16755516827106476\n",
            "count 24096, Loss: 0.22952258586883545\n",
            "count 24128, Loss: 0.1583285927772522\n",
            "count 24160, Loss: 0.2260505110025406\n",
            "count 24192, Loss: 0.1159764975309372\n",
            "count 24224, Loss: 0.24728967249393463\n",
            "count 24256, Loss: 0.1169428825378418\n",
            "count 24288, Loss: 0.0848405659198761\n",
            "count 24320, Loss: 0.11000353842973709\n",
            "count 24352, Loss: 0.1076217070221901\n",
            "count 24384, Loss: 0.0946652889251709\n",
            "count 24416, Loss: 0.20272377133369446\n",
            "count 24448, Loss: 0.20689918100833893\n",
            "count 24480, Loss: 0.26274096965789795\n",
            "count 24512, Loss: 0.09352012723684311\n",
            "count 24544, Loss: 0.10198351740837097\n",
            "count 24576, Loss: 0.11776135861873627\n",
            "count 24608, Loss: 0.08126191049814224\n",
            "count 24640, Loss: 0.10197634994983673\n",
            "count 24672, Loss: 0.021768292412161827\n",
            "count 24704, Loss: 0.14958800375461578\n",
            "count 24736, Loss: 0.04926406964659691\n",
            "count 24768, Loss: 0.05589620769023895\n",
            "count 24800, Loss: 0.3927939236164093\n",
            "count 24832, Loss: 0.03833307698369026\n",
            "count 24864, Loss: 0.07422152906656265\n",
            "count 24896, Loss: 0.011959454976022243\n",
            "count 24928, Loss: 0.25137779116630554\n",
            "count 24960, Loss: 0.01701783947646618\n",
            "count 24992, Loss: 0.26300233602523804\n",
            "count 25024, Loss: 0.6466305255889893\n"
          ]
        }
      ],
      "source": [
        "\"\"\"-------------------------------训练分类器-------------------------------\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "from transformers import BertModel, BertTokenizer, BertForSequenceClassification,AdamW\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from transformers import BertConfig\n",
        "\n",
        "# 实例化模型\n",
        "model = BertTextClassificationModel(num_classes=2)\n",
        "# 定义优化器和损失函数\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch_size= 32\n",
        "# 训练模型\n",
        "model.train()\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")  # 如果有GPU，设备类型设置为\"cuda\"\n",
        "else:\n",
        "    device = torch.device(\"cpu\")   # 如果没有GPU，设备类型设置为\"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# Create a dataset\n",
        "train_dataset = TensorDataset(train_text['input_ids'], train_text['attention_mask'], train_label)\n",
        "\n",
        "# DataLoader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\n",
        "import os\n",
        "for epoch in range(3):\n",
        "    print(\"epoch:\",epoch)\n",
        "    count = 0\n",
        "    loss_list=[]\n",
        "    for data in train_dataloader:\n",
        "        #torch.cuda.empty_cache()\n",
        "        #model.to(device)\n",
        "        count = count+batch_size\n",
        "        input, attention_mask,label =  data\n",
        "        input = input.to(device)\n",
        "        label = label.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input,attention_mask)\n",
        "        #print(outputs)\n",
        "        loss = criterion(outputs, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"count {count}, Loss: {loss.item()}\")\n",
        "        loss_list.append(loss.item())\n",
        "    with open(f'model_{MODEL_NAME}_{MODE}_{epoch+1}_loss.txt', 'w') as file:\n",
        "      for item in loss_list:\n",
        "          file.write(f\"{item}\\n\")\n",
        "    torch.save(model.state_dict(), f'model_{MODEL_NAME}_{MODE}_{epoch+1}.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "jreKCDbHiRpV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "d9eb1216-6b5e-469b-e009-79ed80ea4e11"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-b8589292ebc6>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTextClassificationModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'model_{MODEL_NAME}_{MODE}_{i+1}.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1023\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m                 return _load(opened_zipfile,\n\u001b[0m\u001b[1;32m   1026\u001b[0m                              \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m                              \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1446\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1414\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1416\u001b[0;31m             \u001b[0mtyped_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtyped_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0;31m# stop wrapping with TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m         typed_storage = torch.storage.TypedStorage(\n\u001b[0;32m-> 1390\u001b[0;31m             \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m             _internal=True)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUntypedStorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             untyped_storage = torch.UntypedStorage(\n\u001b[0m\u001b[1;32m    115\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"-------------------------------在测试集上测试训练好的checkpoint-------------------------------\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "\n",
        "acc_list = []\n",
        "loss_list = []\n",
        "#test_data = pd.read_parquet(\"/content/imdb/plain_text/test-00000-of-00001.parquet\")\n",
        "\n",
        "#test_text = tokenizer(test_data['text'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
        "#test_label = torch.tensor(test_data['label'].tolist())\n",
        "for i in range(3):\n",
        "  model = BertTextClassificationModel(num_classes=2)\n",
        "  model.load_state_dict(torch.load(f'model_{MODEL_NAME}_{MODE}_{i+1}.pth'))\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  # Create a dataset\n",
        "  test_dataset = TensorDataset(test_text['input_ids'], test_text['attention_mask'], test_label)\n",
        "\n",
        "  # DataLoader\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "  if torch.cuda.is_available():\n",
        "      device = torch.device(\"cuda\")  # 如果有GPU，设备类型设置为\"cuda\"\n",
        "  else:\n",
        "      device = torch.device(\"cpu\")   # 如果没有GPU，设备类型设置为\"cpu\"\n",
        "  model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "  # Put the model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Initialize variables to keep track of metrics\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  total_loss = 0\n",
        "\n",
        "  # No gradient computation needed during inference\n",
        "  with torch.no_grad():\n",
        "      for batch in test_dataloader:\n",
        "          input_ids, attention_masks, labels = batch\n",
        "          input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)\n",
        "\n",
        "          # Forward pass, get predictions\n",
        "          outputs = model(input_ids, attention_mask=attention_masks)\n",
        "\n",
        "          # Calculate loss if you want to report it\n",
        "          loss = criterion(outputs, labels)\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Convert model logits to class predictions\n",
        "          predictions = torch.argmax(outputs, dim=1)\n",
        "\n",
        "          # Update correct prediction count\n",
        "          correct += (predictions == labels).sum().item()\n",
        "          total += labels.size(0)\n",
        "          print(total)\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = correct / total\n",
        "  model = None\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "  acc_list.append(accuracy)\n",
        "  loss_list.append(total_loss/len(test_dataloader))\n",
        "  print(f\"Accuracy on test data: {accuracy*100:.2f}%\")\n",
        "  print(f\"Average loss on test data: {total_loss/len(test_dataloader):.2f}\")\n",
        "print(acc_list)\n",
        "print(loss_list)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"---------------------------查找pretrain分类失败而finetune分类成功的样本-------------------------------\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")  # 如果有GPU，设备类型设置为\"cuda\"\n",
        "else:\n",
        "    device = torch.device(\"cpu\")   # 如果没有GPU，设备类型设置为\"cpu\"\n",
        "\n",
        "\n",
        "model_pretrained = BertTextClassificationModel(num_classes=2,mode=False)\n",
        "model_pretrained.load_state_dict(torch.load('/content/model_bert-base-uncased_False_2.pth'))\n",
        "model_finetuned = BertTextClassificationModel(num_classes=2,mode=True)\n",
        "model_finetuned.load_state_dict(torch.load('/content/model_bert-base-uncased_True_2.pth'))\n",
        "\n",
        "model_pretrained.to(device)\n",
        "model_pretrained.eval()\n",
        "model_finetuned.to(device)\n",
        "model_finetuned.eval()\n",
        "\n",
        "test_dataset = TensorDataset(test_text['input_ids'], test_text['attention_mask'], test_label)\n",
        "\n",
        "# DataLoader\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1,shuffle=False)\n",
        "count = -1\n",
        "num = 0\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        if(num==2 and count<13000):\n",
        "          count+=1\n",
        "          continue\n",
        "        if(num==4):\n",
        "          break\n",
        "        count+=1\n",
        "        input_ids, attention_masks, labels = batch\n",
        "        input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass, get predictions\n",
        "        outputs_pretrained = model_pretrained(input_ids, attention_mask=attention_masks)\n",
        "\n",
        "\n",
        "        # Convert model logits to class predictions\n",
        "        predictions = torch.argmax(outputs_pretrained, dim=1)\n",
        "\n",
        "        # Update correct prediction count\n",
        "        correct_pretrained = (predictions == labels).sum().item()\n",
        "\n",
        "                # Forward pass, get predictions\n",
        "        outputs_finetuned = model_finetuned(input_ids, attention_mask=attention_masks)\n",
        "\n",
        "\n",
        "        # Convert model logits to class predictions\n",
        "        predictions = torch.argmax(outputs_finetuned, dim=1)\n",
        "\n",
        "        # Update correct prediction count\n",
        "        correct_finetuned = (predictions == labels).sum().item()\n",
        "        if(correct_pretrained==0 and correct_finetuned==1):\n",
        "          print(count)\n",
        "          print(input_ids)\n",
        "          num += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK-ao8g4aeYv",
        "outputId": "6dce01a2-5de6-4f91-f8b0-44916728fb8d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "tensor([[  101,  7527, 13109,  5686, 26730,  2038,  2081,  2070,  1997,  1996,\n",
            "          2190,  2530,  7761,  2840,  2895,  5691,  2412,  2550,  1012,  1999,\n",
            "          3327,  2149, 13945,  1016,  1010,  3147, 11203,  1010,  2569,  2749,\n",
            "          1998,  6151,  2483, 29462,  1016,  2024,  2035,  2895, 10002,  1012,\n",
            "          2017,  2064,  2425,  7527,  2038,  1037,  2613,  6896,  2005,  1996,\n",
            "          6907,  1998,  2010,  3152,  2024,  2467,  2724,  3993,  1010,  5541,\n",
            "          1998,  4629,  3821,  1010,  2007,  2070,  1997,  1996,  2190,  2954,\n",
            "         10071,  2019,  2895,  5470,  2071,  3246,  2005,  1012,  1999,  3327,\n",
            "          2002,  2038,  2179,  1037, 18437,  2007,  3660,  4748, 14322,  1010,\n",
            "          2004, 10904,  2019,  3364,  1998,  2895,  9256,  2004,  2017,  2071,\n",
            "          3246,  2005,  1012,  2023,  2003, 15356,  2041,  2007,  2569,  2749,\n",
            "          1998,  6151,  2483, 29462,  1016,  1010,  2021,  6854,  1996, 11133,\n",
            "          2074,  2987,  1005,  1056,  2444,  2039,  2000,  2037,  7590,  1012,\n",
            "          1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  2045,  2003,\n",
            "          2053,  4797,  2008, 29175, 16872,  3504,  2488,  2182,  2954,  1011,\n",
            "          7968,  2084,  2002,  2038,  2589,  1999,  2086,  1010,  2926,  1999,\n",
            "          1996,  2954,  2002,  2038,  1006,  2005,  3492,  2172,  2053,  3114,\n",
            "          1007,  1999,  1037,  3827,  3526,  1010,  1998,  1999,  1996,  2345,\n",
            "         24419,  2007,  3660,  1010,  2021,  2298,  1999,  2010,  2159,  1012,\n",
            "         29175, 16872,  3849,  2000,  2022,  2757,  2503,  1012,  2045,  1005,\n",
            "          1055,  2498,  1999,  2010,  2159,  2012,  2035,  1012,  2009,  1005,\n",
            "          1055,  2066,  2002,  2074,  2987,  1005,  1056,  2729,  2055,  2505,\n",
            "          2802,  1996,  2878,  2143,  1012,  1998,  2023,  2003,  1996,  2877,\n",
            "          2158,  1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,\n",
            "          2045,  2024,  2060, 26489,  6292,  5919,  2000,  1996,  2143,  1010,\n",
            "          5896,  1011,  7968,  1998, 17453,  1010,  2021,  1996,  2364,  3291,\n",
            "          2003,  2008,  2017,  2024, 12580,  4039,  2000,  7861, 15069,  5562,\n",
            "          2007,  1996,  5394,  1997,  1996,  2143,  1012,  1037, 10218,  9467,\n",
            "          2004,  1045,  2113,  2057,  2035,  2359,  2023,  2143,  2000,  2022,\n",
            "          2004,  2569,  2004,  2009, 15958,  2071,  2031,  2042,  1012,  2045,\n",
            "          2024,  2070,  2204,  9017,  1010,  3262,  1996,  2895,  5019,  3209,\n",
            "          1012,  2023,  2143,  2018,  1037, 27547,  2472,  1998,  2895, 17334,\n",
            "          1010,  1998,  2019, 12476,  7116,  2005, 29175, 16872,  2000,  2227,\n",
            "          2091,  1012,  2023,  2071,  2031,  2042,  1996,  2028,  2000,  3288,\n",
            "          1996,  8003,  2895,  2732,  2067,  2039,  2000, 11969,  1999,  1996,\n",
            "          7395,  1011,  2041,  2895,  3185,  7533,  1012,  1026,  7987,  1013,\n",
            "          1028,  1026,  7987,  1013,  1028, 25664,  1037,  9467,  2008,  2023,\n",
            "          2134,  1005,  1056,  4148,  1012,   102,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0]], device='cuda:0')\n",
            "11\n",
            "tensor([[  101,  6397,  3058,  1006,  3996,  4620,  1010,  4579,  1007,  1010,\n",
            "          2001,  1037, 11519,  2143,  1010,  2021,  1045,  2031,  1037,  2261,\n",
            "          3314,  2007,  2023,  2143,  1012,  2034,  1997,  2035,  1010,  1045,\n",
            "          2123,  1005,  1056,  6346,  1996,  5889,  1999,  2023,  2143,  2012,\n",
            "          2035,  1010,  2021,  2062,  2030,  2625,  1010,  1045,  2031,  1037,\n",
            "          3291,  2007,  1996,  5896,  1012,  2036,  1010,  1045,  3305,  2008,\n",
            "          2023,  2143,  2001,  2081,  1999,  1996,  4479,  1005,  1055,  1998,\n",
            "          2111,  2020,  2559,  2000,  4019,  4507,  1010,  2021,  1996,  5896,\n",
            "          2081,  5754,  2061, 12399,  2078,  1005,  1055,  2839,  2298,  5410,\n",
            "          1012,  2016,  2921,  2183,  2067,  1998,  5743,  2090,  4848,  5668,\n",
            "          1998,  1045,  2371,  2004,  2295,  2016,  2323,  2031,  4370,  2007,\n",
            "          2703,  5163,  1005,  1055,  2839,  1999,  1996,  2203,  1012,  2002,\n",
            "          5621,  2106,  2729,  2055,  2014,  1998,  2014,  2155,  1998,  2052,\n",
            "          2031,  2589,  2505,  2005,  2014,  1998,  2002,  2106,  2011,  3228,\n",
            "          2014,  2039,  1999,  1996,  2203,  2000, 10882, 19250,  6606,  5226,\n",
            "          2040,  1999,  2026,  5448,  2001,  2069,  2041,  2005,  1037,  2204,\n",
            "          2051,  1012,  2703,  5163,  1005,  1055,  2839,  1010,  2348,  1037,\n",
            "          2147,  4430, 23518,  2001,  1037,  2158,  1997, 11109,  1998,  5621,\n",
            "          3866, 14433,  1006,  5754,  2061, 12399,  2078,  1007,  2004,  4941,\n",
            "          2000,  6606,  5226,  1010,  2096,  2002,  2106,  2066,  2014,  1037,\n",
            "          2843,  1010,  1045,  2134,  1005,  1056,  2156,  1996,  5995,  1997,\n",
            "          2293,  2008,  2002,  2018,  2005,  2014,  2839,  1012,  1996,  2537,\n",
            "          5300,  2020,  2307,  1010,  2021,  1996,  5896,  2071,  2031,  2109,\n",
            "          1037,  2210,  2147,  1012,   102,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0]], device='cuda:0')\n",
            "13011\n",
            "tensor([[  101,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  1000,\n",
            "         11082, 19948,  9916,  1011,  2115,  2564,  1010,  2026,  2269,  1000,\n",
            "          1011,  9428,  7036,  4512,  2090,  2048, 12358,  1011, 10391,  4938,\n",
            "          1998,  3124, 15030,  5267,  2043,  2027,  3113,  2058,  6265,  2006,\n",
            "          1037,  3345,  4990,  1012,  3124,  1010,  1037,  5024,  1010, 19416,\n",
            "          5093,  2447,  1010,  3005,  3291,  2003,  2008,  2010,  2564,  1010,\n",
            "          1996, 27978, 10450,  3560, 16925,  1010,  2180,  1005,  1056,  8179,\n",
            "          2032,  2061,  2002,  2064,  5914, 10153,  2684,  4776,  1010, 11680,\n",
            "          1996,  2878,  4512,  2125,  2004,  1037,  8257,  1012,  1996,  2206,\n",
            "          2733,  2002,  3475,  1005,  1056,  5870,  2151,  2062,  1012,  1999,\n",
            "          1037,  3496,  1997,  4438, 19625, 23873,  1010, 10391, 29594, 16925,\n",
            "          2083,  1037, 11485,  1998,  2358, 21476,  2015,  2014,  1012,  2004,\n",
            "          2002,  2515,  1010,  2014,  7877,  2991,  2125,  1998,  2057,  2156,\n",
            "          1996,  4028, 25212, 11272,  7686,  3807,  2083,  2014, 15072,  1012,\n",
            "          3147, 18627,  1998, 16095,  2389, 10391,  1010,  2010,  2112,  1997,\n",
            "          1996,  3066,  2949,  1010,  8107,  2019, 29279,  3124,  8074,  1010,\n",
            "          2130,  2811, 12228,  2032,  2046,  1005,  2725,  2010,  2978,  1012,\n",
            "          1005,  5609,  2024,  2025,  3271,  2043,  4776,  1005,  1055,  3653,\n",
            "          3597, 18436,  1998, 22430,  3920,  2905,  4332,  2039,  8343,  2075,\n",
            "          3124,  1997, 16925,  1005,  1055,  4028,  1012,  2061,  5496,  1997,\n",
            "          1037,  4028,  2002,  2134,  1005,  1056, 10797,  1998,  3517,  2000,\n",
            "         10797,  2178,  1010,  2054,  2003,  3124,  2183,  2000,  2079,  1029,\n",
            "          1996,  2373,  1997,  2023,  2143,  2003,  1999,  1996,  8312,  1997,\n",
            "          2529,  9552,  2004,  2383,  1037, 25303,  2217,  2000,  2037,  3267,\n",
            "          1011,  1998,  2023, 19625,  2515,  2000, 15401,  1012,   102,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0]], device='cuda:0')\n",
            "13023\n",
            "tensor([[  101,  1996,  5436,  2003,  5399,  2434,  1010,  1998,  2035,  1996,\n",
            "          5889,  2106,  2037,  3105,  3492,  2092,  1012,  2045,  2003,  1037,\n",
            "          7564,  1997,  5021,  4335,  1010,  2205,  1012,  2070,  2477,  2079,\n",
            "          2025,  2191,  1037,  2843,  1997,  3168,  1006,  1041,  1012,  1043,\n",
            "          1012,  1996,  2034,  1000,  5252,  1000,  3496,  1010,  2339,  2052,\n",
            "          1996, 18869,  2025,  2074,  5342,  4873,  1998,  3524,  2127,  1996,\n",
            "          2919, 18389,  2681,  1029,  1007,  1012,  1996, 12513,  2941,  3713,\n",
            "          2845,  2302,  9669,  1010,  2021,  1996,  3765,  2024,  5186,  4326,\n",
            "          1010,  2007,  4275, 13912,  2066,  2753,  2086,  1010,  1998,  2061,\n",
            "          1996,  2173,  3849,  6881,  2135,  2041,  1997,  2051,  1012,  3452,\n",
            "          1010,  2065,  2017,  2066, 10874,  2015,  2017,  2097,  2763,  2066,\n",
            "          2023,  2028,  2004,  2092,  1012,   102,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"-------------------------------test-1-------------------------------\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "text = \"\"\"When I first saw a glimpse of this movie, I quickly noticed the actress who was playing the role of Lucille Ball. Rachel York's portrayal of Lucy is absolutely awful. Lucille Ball was an astounding comedian with incredible talent. To think about a legend like Lucille Ball being portrayed the way she was in the movie is horrendous. I cannot believe out of all the actresses in the world who could play a much better Lucy, the producers decided to get Rachel York. She might be a good actress in other roles but to play the role of Lucille Ball is tough. It is pretty hard to find someone who could resemble Lucille Ball, but they could at least find someone a bit similar in looks and talent. If you noticed York's portrayal of Lucy in episodes of I Love Lucy like the chocolate factory or vitavetavegamin, nothing is similar in any way-her expression, voice, or movement.<br /><br />To top it all off, Danny Pino playing Desi Arnaz is horrible. Pino does not qualify to play as Ricky. He's small and skinny, his accent is unreal, and once again, his acting is unbelievable. Although Fred and Ethel were not similar either, they were not as bad as the characters of Lucy and Ricky.<br /><br />Overall, extremely horrible casting and the story is badly told. If people want to understand the real life situation of Lucille Ball, I suggest watching A&E Biography of Lucy and Desi, read the book from Lucille Ball herself, or PBS' American Masters: Finding Lucy. If you want to see a docudrama, \"Before the Laughter\" would be a better choice. The casting of Lucille Ball and Desi Arnaz in \"Before the Laughter\" is much better compared to this. At least, a similar aspect is shown rather than nothing.\"\"\"\n",
        "token = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
        "print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4DMbSzUrsPJ",
        "outputId": "6330ab44-4e3f-45da-cf34-d3c60ddbcc21"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  2043,  1045,  2034,  2387,  1037, 12185,  1997,  2023,  3185,\n",
            "          1010,  1045,  2855,  4384,  1996,  3883,  2040,  2001,  2652,  1996,\n",
            "          2535,  1997, 28016,  3608,  1012,  5586,  2259,  1005,  1055, 13954,\n",
            "          1997,  7004,  2003,  7078,  9643,  1012, 28016,  3608,  2001,  2019,\n",
            "          2004, 24826, 15683,  9971,  2007,  9788,  5848,  1012,  2000,  2228,\n",
            "          2055,  1037,  5722,  2066, 28016,  3608,  2108,  6791,  1996,  2126,\n",
            "          2016,  2001,  1999,  1996,  3185,  2003,  7570, 14343, 15482,  2271,\n",
            "          1012,  1045,  3685,  2903,  2041,  1997,  2035,  1996, 19910,  1999,\n",
            "          1996,  2088,  2040,  2071,  2377,  1037,  2172,  2488,  7004,  1010,\n",
            "          1996,  6443,  2787,  2000,  2131,  5586,  2259,  1012,  2016,  2453,\n",
            "          2022,  1037,  2204,  3883,  1999,  2060,  4395,  2021,  2000,  2377,\n",
            "          1996,  2535,  1997, 28016,  3608,  2003,  7823,  1012,  2009,  2003,\n",
            "          3492,  2524,  2000,  2424,  2619,  2040,  2071, 13014, 28016,  3608,\n",
            "          1010,  2021,  2027,  2071,  2012,  2560,  2424,  2619,  1037,  2978,\n",
            "          2714,  1999,  3504,  1998,  5848,  1012,  2065,  2017,  4384,  2259,\n",
            "          1005,  1055, 13954,  1997,  7004,  1999,  4178,  1997,  1045,  2293,\n",
            "          7004,  2066,  1996,  7967,  4713,  2030, 19300, 19510, 10696, 26517,\n",
            "          2078,  1010,  2498,  2003,  2714,  1999,  2151,  2126,  1011,  2014,\n",
            "          3670,  1010,  2376,  1010,  2030,  2929,  1012,  1026,  7987,  1013,\n",
            "          1028,  1026,  7987,  1013,  1028,  2000,  2327,  2009,  2035,  2125,\n",
            "          1010,  6266,  9231,  2080,  2652,  4078,  2072, 12098,  2532,  2480,\n",
            "          2003,  9202,  1012,  9231,  2080,  2515,  2025,  7515,  2000,  2377,\n",
            "          2004, 11184,  1012,  2002,  1005,  1055,  2235,  1998, 15629,  1010,\n",
            "          2010,  9669,  2003,  4895, 22852,  1010,  1998,  2320,  2153,  1010,\n",
            "          2010,  3772,  2003, 23653,  1012,  2348,  5965,  1998, 19180,  2020,\n",
            "          2025,  2714,  2593,  1010,  2027,  2020,  2025,  2004,  2919,  2004,\n",
            "          1996,  3494,  1997,  7004,  1998, 11184,  1012,  1026,  7987,  1013,\n",
            "          1028,  1026,  7987,  1013,  1028,  3452,  1010,  5186,  9202,  9179,\n",
            "          1998,  1996,  2466,  2003,  6649,  2409,  1012,  2065,  2111,  2215,\n",
            "          2000,  3305,  1996,  2613,  2166,  3663,  1997, 28016,  3608,  1010,\n",
            "          1045,  6592,  3666,  1037,  1004,  1041,  8308,  1997,  7004,  1998,\n",
            "          4078,  2072,  1010,  3191,  1996,  2338,  2013, 28016,  3608,  2841,\n",
            "          1010,  2030, 13683,  1005,  2137,  5972,  1024,  4531,  7004,  1012,\n",
            "          2065,  2017,  2215,  2000,  2156,  1037,  9986,  6784, 14672,  1010,\n",
            "          1000,  2077,  1996,  7239,  1000,  2052,  2022,  1037,  2488,  3601,\n",
            "          1012,  1996,  9179,  1997, 28016,  3608,  1998,  4078,  2072, 12098,\n",
            "          2532,  2480,  1999,  1000,  2077,  1996,  7239,  1000,  2003,  2172,\n",
            "          2488,  4102,  2000,  2023,  1012,  2012,  2560,  1010,  1037,  2714,\n",
            "          7814,  2003,  3491,  2738,  2084,  2498,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"-------------------------------test-2-------------------------------\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "MODEL_NAME = \"bert-base-uncased\"\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
        "import pandas as pd\n",
        "model=BertModel.from_pretrained(MODEL_NAME)\n",
        "output=model(input_ids=[1,0],attention_mask=[1,1])\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "_RKuJ4HnkdV0",
        "outputId": "e22ce300-6052-4ebc-fb0a-956288a82b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'size'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6157777d24a6>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1051\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn_if_padding_and_no_attention_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1053\u001b[0;31m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1054\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"-------------------------------test-3-------------------------------\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"inference by pretrained or finetuned\"\"\"\n",
        "\n",
        "MODEL_NAME = \"bert-base-uncased\"\n",
        "import pandas as pd\n",
        "\n",
        "# 代码示例：加载预训练的BERT模型\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# 加载BERT的tokenizer和预训练模型\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "\n",
        "train_data = pd.read_parquet(\"/content/imdb/plain_text/train-00000-of-00001.parquet\")\n",
        "\n",
        "train_text = tokenizer(train_data['text'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
        "train_label = torch.tensor(train_data['label'].tolist())\n",
        "\n",
        "test_data = pd.read_parquet(\"/content/imdb/plain_text/test-00000-of-00001.parquet\")\n",
        "\n",
        "test_text = tokenizer(test_data['text'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
        "test_label = torch.tensor(test_data['label'].tolist())"
      ],
      "metadata": {
        "id": "9viV_wZczMWC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}