# gpt提供的分析错误样本的方法
明白了，您想要可视化的是最终分类结果对每个输入词的注意力，而不是句子中词与词之间的注意力。这种情况可以通过基于梯度的方法或 attention roll-out 方法来实现。

### 基于梯度的方法

这种方法通过计算模型输出（分类结果）相对于输入词嵌入的梯度，来评估每个输入词对最终输出的影响。

### 示例代码

以下是一个使用基于梯度的方法来计算并可视化每个输入词对最终分类结果影响的示例：

```python
import torch
import torch.nn as nn
from transformers import BertTokenizer, BertModel, BertConfig
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

MODEL_NAME = "bert-base-uncased"
MODE = False  # 是否冻结参数

class BertTextClassificationModel(nn.Module):
    def __init__(self, num_classes=2, model_name=MODEL_NAME, dropout_prob=0.1):
        super(BertTextClassificationModel, self).__init__()
        self.bert = BertModel.from_pretrained(model_name, output_attentions=False)
        self.dropout = nn.Dropout(dropout_prob)
        self.classifier = nn.Sequential(
            nn.Linear(self.bert.config.hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(dropout_prob),
            nn.Linear(256, num_classes)
        )

        # 冻结BERT模型的参数
        for param in self.bert.parameters():
            param.requires_grad = MODE

    def forward(self, input_ids, attention_mask, token_type_ids=None):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        last_hidden_state = outputs.last_hidden_state
        pooled_output = torch.mean(last_hidden_state, dim=1)

        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        return logits, last_hidden_state

# 加载模型和tokenizer
tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)
model = BertTextClassificationModel()
model.load_state_dict(torch.load('/path/to/checkpoint.pth'))
model.eval()

# 示例输入
sample_text = "This is a sample text for testing."
inputs = tokenizer(sample_text, return_tensors='pt', padding=True, truncation=True)

# 前向传播获取 logits 和 last_hidden_state
inputs['input_ids'].requires_grad = True
logits, last_hidden_state = model(inputs['input_ids'], inputs['attention_mask'])

# 获取目标类别
target_class = torch.argmax(logits, dim=1)

# 计算目标类别相对于输入词嵌入的梯度
logits[:, target_class].backward()

# 获取输入词嵌入的梯度
gradients = inputs['input_ids'].grad[0].cpu().numpy()

# 计算每个词的影响力（梯度的绝对值之和）
influence = np.sum(np.abs(gradients), axis=1)

# 可视化每个词的影响力
tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
plt.figure(figsize=(10, 5))
sns.barplot(x=tokens, y=influence, palette="viridis")
plt.xticks(rotation=90)
plt.xlabel('Tokens')
plt.ylabel('Influence')
plt.title('Influence of Each Token on the Classification Result')
plt.show()
```

### 解释

1. **模型定义**：
    - 定义 `BertTextClassificationModel` 类，不需要 `output_attentions` 参数，因为我们不再需要内部注意力权重。

2. **加载模型和 tokenizer**：
    - 加载预训练模型和 tokenizer。
    - 加载之前保存的 checkpoint。

3. **计算梯度**：
    - 设置 `inputs['input_ids'].requires_grad = True` 以使输入词嵌入的梯度可计算。
    - 前向传播得到 logits 和 `last_hidden_state`。
    - 使用 `logits[:, target_class].backward()` 计算目标类别相对于输入词嵌入的梯度。
    - 提取梯度并计算每个词的影响力。

4. **可视化**：
    - 使用 seaborn 绘制每个词的影响力。

### 注意事项

- 确保模型的架构与保存 checkpoint 时的架构一致。
- 计算梯度时使用 `.backward()`，并提取 `inputs['input_ids']` 的梯度。
- 使用梯度的绝对值之和来表示每个词的影响力。

通过这些步骤，您可以得到最终分类结果对每个输入词的影响力，并进行可视化。

'6'表示测试集第7个数据，此数据被base的pretrained版本判断失败，被finetuned判断成功
6
tensor([[  101,  7527, 13109,  5686, 26730,  2038,  2081,  2070,  1997,  1996,
          2190,  2530,  7761,  2840,  2895,  5691,  2412,  2550,  1012,  1999,
          3327,  2149, 13945,  1016,  1010,  3147, 11203,  1010,  2569,  2749,
          1998,  6151,  2483, 29462,  1016,  2024,  2035,  2895, 10002,  1012,
          2017,  2064,  2425,  7527,  2038,  1037,  2613,  6896,  2005,  1996,
          6907,  1998,  2010,  3152,  2024,  2467,  2724,  3993,  1010,  5541,
          1998,  4629,  3821,  1010,  2007,  2070,  1997,  1996,  2190,  2954,
         10071,  2019,  2895,  5470,  2071,  3246,  2005,  1012,  1999,  3327,
          2002,  2038,  2179,  1037, 18437,  2007,  3660,  4748, 14322,  1010,
          2004, 10904,  2019,  3364,  1998,  2895,  9256,  2004,  2017,  2071,
          3246,  2005,  1012,  2023,  2003, 15356,  2041,  2007,  2569,  2749,
          1998,  6151,  2483, 29462,  1016,  1010,  2021,  6854,  1996, 11133,
          2074,  2987,  1005,  1056,  2444,  2039,  2000,  2037,  7590,  1012,
          1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  2045,  2003,
          2053,  4797,  2008, 29175, 16872,  3504,  2488,  2182,  2954,  1011,
          7968,  2084,  2002,  2038,  2589,  1999,  2086,  1010,  2926,  1999,
          1996,  2954,  2002,  2038,  1006,  2005,  3492,  2172,  2053,  3114,
          1007,  1999,  1037,  3827,  3526,  1010,  1998,  1999,  1996,  2345,
         24419,  2007,  3660,  1010,  2021,  2298,  1999,  2010,  2159,  1012,
         29175, 16872,  3849,  2000,  2022,  2757,  2503,  1012,  2045,  1005,
          1055,  2498,  1999,  2010,  2159,  2012,  2035,  1012,  2009,  1005,
          1055,  2066,  2002,  2074,  2987,  1005,  1056,  2729,  2055,  2505,
          2802,  1996,  2878,  2143,  1012,  1998,  2023,  2003,  1996,  2877,
          2158,  1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,
          2045,  2024,  2060, 26489,  6292,  5919,  2000,  1996,  2143,  1010,
          5896,  1011,  7968,  1998, 17453,  1010,  2021,  1996,  2364,  3291,
          2003,  2008,  2017,  2024, 12580,  4039,  2000,  7861, 15069,  5562,
          2007,  1996,  5394,  1997,  1996,  2143,  1012,  1037, 10218,  9467,
          2004,  1045,  2113,  2057,  2035,  2359,  2023,  2143,  2000,  2022,
          2004,  2569,  2004,  2009, 15958,  2071,  2031,  2042,  1012,  2045,
          2024,  2070,  2204,  9017,  1010,  3262,  1996,  2895,  5019,  3209,
          1012,  2023,  2143,  2018,  1037, 27547,  2472,  1998,  2895, 17334,
          1010,  1998,  2019, 12476,  7116,  2005, 29175, 16872,  2000,  2227,
          2091,  1012,  2023,  2071,  2031,  2042,  1996,  2028,  2000,  3288,
          1996,  8003,  2895,  2732,  2067,  2039,  2000, 11969,  1999,  1996,
          7395,  1011,  2041,  2895,  3185,  7533,  1012,  1026,  7987,  1013,
          1028,  1026,  7987,  1013,  1028, 25664,  1037,  9467,  2008,  2023,
          2134,  1005,  1056,  4148,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0]], device='cuda:0')
11
tensor([[  101,  6397,  3058,  1006,  3996,  4620,  1010,  4579,  1007,  1010,
          2001,  1037, 11519,  2143,  1010,  2021,  1045,  2031,  1037,  2261,
          3314,  2007,  2023,  2143,  1012,  2034,  1997,  2035,  1010,  1045,
          2123,  1005,  1056,  6346,  1996,  5889,  1999,  2023,  2143,  2012,
          2035,  1010,  2021,  2062,  2030,  2625,  1010,  1045,  2031,  1037,
          3291,  2007,  1996,  5896,  1012,  2036,  1010,  1045,  3305,  2008,
          2023,  2143,  2001,  2081,  1999,  1996,  4479,  1005,  1055,  1998,
          2111,  2020,  2559,  2000,  4019,  4507,  1010,  2021,  1996,  5896,
          2081,  5754,  2061, 12399,  2078,  1005,  1055,  2839,  2298,  5410,
          1012,  2016,  2921,  2183,  2067,  1998,  5743,  2090,  4848,  5668,
          1998,  1045,  2371,  2004,  2295,  2016,  2323,  2031,  4370,  2007,
          2703,  5163,  1005,  1055,  2839,  1999,  1996,  2203,  1012,  2002,
          5621,  2106,  2729,  2055,  2014,  1998,  2014,  2155,  1998,  2052,
          2031,  2589,  2505,  2005,  2014,  1998,  2002,  2106,  2011,  3228,
          2014,  2039,  1999,  1996,  2203,  2000, 10882, 19250,  6606,  5226,
          2040,  1999,  2026,  5448,  2001,  2069,  2041,  2005,  1037,  2204,
          2051,  1012,  2703,  5163,  1005,  1055,  2839,  1010,  2348,  1037,
          2147,  4430, 23518,  2001,  1037,  2158,  1997, 11109,  1998,  5621,
          3866, 14433,  1006,  5754,  2061, 12399,  2078,  1007,  2004,  4941,
          2000,  6606,  5226,  1010,  2096,  2002,  2106,  2066,  2014,  1037,
          2843,  1010,  1045,  2134,  1005,  1056,  2156,  1996,  5995,  1997,
          2293,  2008,  2002,  2018,  2005,  2014,  2839,  1012,  1996,  2537,
          5300,  2020,  2307,  1010,  2021,  1996,  5896,  2071,  2031,  2109,
          1037,  2210,  2147,  1012,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0]], device='cuda:0')
13011
tensor([[  101,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  1000,
         11082, 19948,  9916,  1011,  2115,  2564,  1010,  2026,  2269,  1000,
          1011,  9428,  7036,  4512,  2090,  2048, 12358,  1011, 10391,  4938,
          1998,  3124, 15030,  5267,  2043,  2027,  3113,  2058,  6265,  2006,
          1037,  3345,  4990,  1012,  3124,  1010,  1037,  5024,  1010, 19416,
          5093,  2447,  1010,  3005,  3291,  2003,  2008,  2010,  2564,  1010,
          1996, 27978, 10450,  3560, 16925,  1010,  2180,  1005,  1056,  8179,
          2032,  2061,  2002,  2064,  5914, 10153,  2684,  4776,  1010, 11680,
          1996,  2878,  4512,  2125,  2004,  1037,  8257,  1012,  1996,  2206,
          2733,  2002,  3475,  1005,  1056,  5870,  2151,  2062,  1012,  1999,
          1037,  3496,  1997,  4438, 19625, 23873,  1010, 10391, 29594, 16925,
          2083,  1037, 11485,  1998,  2358, 21476,  2015,  2014,  1012,  2004,
          2002,  2515,  1010,  2014,  7877,  2991,  2125,  1998,  2057,  2156,
          1996,  4028, 25212, 11272,  7686,  3807,  2083,  2014, 15072,  1012,
          3147, 18627,  1998, 16095,  2389, 10391,  1010,  2010,  2112,  1997,
          1996,  3066,  2949,  1010,  8107,  2019, 29279,  3124,  8074,  1010,
          2130,  2811, 12228,  2032,  2046,  1005,  2725,  2010,  2978,  1012,
          1005,  5609,  2024,  2025,  3271,  2043,  4776,  1005,  1055,  3653,
          3597, 18436,  1998, 22430,  3920,  2905,  4332,  2039,  8343,  2075,
          3124,  1997, 16925,  1005,  1055,  4028,  1012,  2061,  5496,  1997,
          1037,  4028,  2002,  2134,  1005,  1056, 10797,  1998,  3517,  2000,
         10797,  2178,  1010,  2054,  2003,  3124,  2183,  2000,  2079,  1029,
          1996,  2373,  1997,  2023,  2143,  2003,  1999,  1996,  8312,  1997,
          2529,  9552,  2004,  2383,  1037, 25303,  2217,  2000,  2037,  3267,
          1011,  1998,  2023, 19625,  2515,  2000, 15401,  1012,   102,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0]], device='cuda:0')
13023
tensor([[  101,  1996,  5436,  2003,  5399,  2434,  1010,  1998,  2035,  1996,
          5889,  2106,  2037,  3105,  3492,  2092,  1012,  2045,  2003,  1037,
          7564,  1997,  5021,  4335,  1010,  2205,  1012,  2070,  2477,  2079,
          2025,  2191,  1037,  2843,  1997,  3168,  1006,  1041,  1012,  1043,
          1012,  1996,  2034,  1000,  5252,  1000,  3496,  1010,  2339,  2052,
          1996, 18869,  2025,  2074,  5342,  4873,  1998,  3524,  2127,  1996,
          2919, 18389,  2681,  1029,  1007,  1012,  1996, 12513,  2941,  3713,
          2845,  2302,  9669,  1010,  2021,  1996,  3765,  2024,  5186,  4326,
          1010,  2007,  4275, 13912,  2066,  2753,  2086,  1010,  1998,  2061,
          1996,  2173,  3849,  6881,  2135,  2041,  1997,  2051,  1012,  3452,
          1010,  2065,  2017,  2066, 10874,  2015,  2017,  2097,  2763,  2066,
          2023,  2028,  2004,  2092,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0]], device='cuda:0')