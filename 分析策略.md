# gpt提供的分析错误样本的方法
明白了，您想要可视化的是最终分类结果对每个输入词的注意力，而不是句子中词与词之间的注意力。这种情况可以通过基于梯度的方法或 attention roll-out 方法来实现。

### 基于梯度的方法

这种方法通过计算模型输出（分类结果）相对于输入词嵌入的梯度，来评估每个输入词对最终输出的影响。

### 示例代码

以下是一个使用基于梯度的方法来计算并可视化每个输入词对最终分类结果影响的示例：

```python
import torch
import torch.nn as nn
from transformers import BertTokenizer, BertModel, BertConfig
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

MODEL_NAME = "bert-base-uncased"
MODE = False  # 是否冻结参数

class BertTextClassificationModel(nn.Module):
    def __init__(self, num_classes=2, model_name=MODEL_NAME, dropout_prob=0.1):
        super(BertTextClassificationModel, self).__init__()
        self.bert = BertModel.from_pretrained(model_name, output_attentions=False)
        self.dropout = nn.Dropout(dropout_prob)
        self.classifier = nn.Sequential(
            nn.Linear(self.bert.config.hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(dropout_prob),
            nn.Linear(256, num_classes)
        )

        # 冻结BERT模型的参数
        for param in self.bert.parameters():
            param.requires_grad = MODE

    def forward(self, input_ids, attention_mask, token_type_ids=None):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        last_hidden_state = outputs.last_hidden_state
        pooled_output = torch.mean(last_hidden_state, dim=1)

        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)
        return logits, last_hidden_state

# 加载模型和tokenizer
tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)
model = BertTextClassificationModel()
model.load_state_dict(torch.load('model_checkpoint.pth'))
model.eval()

# 示例输入
sample_text = "This is a sample text for testing."
inputs = tokenizer(sample_text, return_tensors='pt', padding=True, truncation=True)

# 前向传播获取 logits 和 last_hidden_state
inputs['input_ids'].requires_grad = True
logits, last_hidden_state = model(inputs['input_ids'], inputs['attention_mask'])

# 获取目标类别
target_class = torch.argmax(logits, dim=1)

# 计算目标类别相对于输入词嵌入的梯度
logits[:, target_class].backward()

# 获取输入词嵌入的梯度
gradients = inputs['input_ids'].grad[0].cpu().numpy()

# 计算每个词的影响力（梯度的绝对值之和）
influence = np.sum(np.abs(gradients), axis=1)

# 可视化每个词的影响力
tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
plt.figure(figsize=(10, 5))
sns.barplot(x=tokens, y=influence, palette="viridis")
plt.xticks(rotation=90)
plt.xlabel('Tokens')
plt.ylabel('Influence')
plt.title('Influence of Each Token on the Classification Result')
plt.show()
```

### 解释

1. **模型定义**：
    - 定义 `BertTextClassificationModel` 类，不需要 `output_attentions` 参数，因为我们不再需要内部注意力权重。

2. **加载模型和 tokenizer**：
    - 加载预训练模型和 tokenizer。
    - 加载之前保存的 checkpoint。

3. **计算梯度**：
    - 设置 `inputs['input_ids'].requires_grad = True` 以使输入词嵌入的梯度可计算。
    - 前向传播得到 logits 和 `last_hidden_state`。
    - 使用 `logits[:, target_class].backward()` 计算目标类别相对于输入词嵌入的梯度。
    - 提取梯度并计算每个词的影响力。

4. **可视化**：
    - 使用 seaborn 绘制每个词的影响力。

### 注意事项

- 确保模型的架构与保存 checkpoint 时的架构一致。
- 计算梯度时使用 `.backward()`，并提取 `inputs['input_ids']` 的梯度。
- 使用梯度的绝对值之和来表示每个词的影响力。

通过这些步骤，您可以得到最终分类结果对每个输入词的影响力，并进行可视化。